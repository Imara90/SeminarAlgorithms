\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{fixltx2e}

\usepackage{xcolor}
\newcommand\myworries[1]{\textcolor{red}{#1}}
\newcommand\todo[1]{\textcolor{blue}{#1}}

\usepackage{url}
\urldef{\mailsa}\path|{i.c.t.m.speek, a.c.stolwijk}@student.tudelft.nl|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip%
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter% start of an individual contribution

% first the title is needed
\title{Realistically Interesting Domains for Probabilistic Planners}

% a short form should be given in case it is too long for the running head
\titlerunning{Real World Domains}

\author{I.C.T.M Speek, A.C. Stolwijk}

%
\authorrunning{Validating Planners in a Real World Domain}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Seminar Algorithms, Embedded Software,
MSc Embedded Systems,\\
Delft University of Technology\\
\mailsa\\
}

%\toctitle{Lecture Notes in Computer Science}
%\tocauthor{Authors' Instructions}

\maketitle

%--------------------------------------------------------------------

\begin{abstract}
	In contrast to current demand for probabilistically interesting domains in the International Probabilistic Planning Competitions, this papier argues for using real-world appropriate domains. By relaxing the definition of a dead-end, we argue that for many
real life domains a simpler planning algorithm can be sufficient and even better than sophisticated probabilistic planners.

In this paper we evaluate the strength of FF-Replan, winner of IPPC-04 and unenrolled winner of IPPC-06, a replanner that uses the classical planner FF on a determinized probabilistic problem against a state of the art probabilistic planner \textsc{Gourmand}, the successor of the Labelled Real-Time Dynamic Probabilistic planner \textsc{Glutton}, runner up of the IPPC-11. By translating the Navigation and Traffic domain from IPPC-11 to a matching real-world equivalent and evaluating these using the proposed probabilistically interesting baseline test as proposed by Little et al\. we evaluate whether FF-Replan is in fact capable enough to solve enough of these so called classical planning problems in disguise.
\end{abstract}

%--------------------------------------------------------------------

\section{Introduction}

Since the first the International Probabilistic Planning Competition (IPPC),
problems have undergone radical changes. Dominating planners of previous
incorporations had a strong focus on goal-oriented MDPs, determinizing and
using powerful classical planners (e.g. FF \cite{Hoffmann01theff}) and
replanning. New benchmark problems have demonstrated weaknesses in existing
solution techniques.

Little and Thi\'ebaux have developed a baseline test in
\cite{little2007probvsreplan} to evaluate the problems of the fifth International
Planning Competition (IPC-5). This test determines if a planning problem is
`probabilistically interesting'. Problems that fail this test are classical
planning problems in disguise. Knowing this it is easy to design a problem
where a replanner performs badly or incredibly well.

The IPPC-11 introduced Relation Dynamic Influence Diagram Language (RDDL),
which can design domains that are more probabilistically interesting. The top
performers were PROST \cite{keller2012prost} and \textsc{Glutton}
\cite{kolobov2012glutton}.

While it is interesting to make problems harder and invent better planning
algorithms to solve those probabilistic complex problems, we evaluate whether
or not these problems are coherent with real-world situations.

The main contribution of this paper is an evaluation of FF-Replan versus a
probabilistically state of the art planner \textsc{Gourmand} in the realistically
augmented domains of IPPC-11. If these domains fail the baseline test proposed
by Little et al\., they would translate to classical planning problems in
disguise and could be easily solved by FF-Replan.

%\begin{itemize}
%	\item Describe probabilistic planning problem
%	\item Introduce planning for uncertainty
%	\item Describe how deterministic classical planners can be used in probabilistic domains and motivate why this is interesting
%	\item Introduce the qualities of replanning and the second planning algorithm
%	\item Describe the need for a new domain for real world applications
%	\item Describe the goal and tools used to reach this goal
%\end{itemize}

%---------------------------------------------------------------------

\section{Problem formulation}

To evaluate FF-Replan and \textsc{Gourmand} in a real world domain a finite horizon MDP is used to represent the realistic \emph{natural dynamics} such as external forces or forces of nature. These dynamics introduce large branching factors emphasizing the need for a planner to be scalable. To evaluate the model, the planner should also be sensitive to an expected reward maximization criterion.
Current finite horizon MDP problems as introduced in IPPC-11 are simplified and therefor do not account for the rich set of state-actions as would be available in a real-world planning problem. We will reason about the most interesting domains for our evaluation and adjust them to match their real-world equivalent. Using the baseline test introduced by Little et al.\@ \cite{little2007probvsreplan}, we can evaluate the probabilistically interestingness of these domains and evaluate the performance of FF-Replan vs.\@ a state of the art probabilistic planner: \textsc{Gourmand}.



%---------------------------------------------------------------------

\section{Background}
\label{sec:background}

\subsection{Planning problems}
\subsubsection{Classical Planning problems. } Planning is the process of computing several steps of a problem-solving procedure before executing any of them. In classical planning problems the environment is assumed to \emph{deterministic, observable and state}. Classical planning problems consists out of states, actions, conditions and a goal state and are solved using search and can be aided using heuristics. The planner tries to find a sequence of actions such that a goal or maximum reward is achieved.

\subsubsection{Probabilistic Planning problems}
Real-World applications are far from deterministic, induced by natural dynamics and chance. To accommodate for Real-World domains probabilistic planning problems were introduced.  These systems can be described using \emph{Markov Decision Processes} (MDPs) and can account for observable uncertainty. Probabilistic planning problems can be solved by evaluating iterations and creating policies to reach a goal state, either online or offline.

\subsection{Problem domains}
\subsubsection{Goal-oriented MDPs. }The problems in IPPC-04 were specified using the Planning Domain Description Language (PPDLL). PPDDL specifies a probabistic planning problem as a set of probabilistic action sets and an initial and goal state description in which every action has a certain probability assigned to it \cite{FFReplan}. These MDPs are of the form $\mathit{M = (S, A, app, Pr, R)}$ where $\mathit{S}$ and $\mathit{A}$ are a finite set of states and actions, $\mathit{app(s)}$ is the set of all actions applicable in $\mathit{s}$. $\mathit{Pr}$ represents the probability of a state transaction $\mathit{(s, a, s')}$ for $\mathit{a}$ and $\mathit{R(s, a, s')}$ is the reward of applying $\mathit{a}$ to $\mathit{s}$ and generating $\mathit{s'}$ The planning problem is defined as a triple $\mathit{P = (M, s\textsubscript{0}, G, \rho)}$ where $\mathit{M = (S, A, app, Pr, R)}$ is an MDP, $\mathit{s\textsubscript{0} \in S}$ is the initial state, $\mathit{G \subseteq S}$ is the set of goal states and $\mathit{0 < \rho < 1}$ is a probability threshold \cite{teichteil2010incremental}.

 The planners from IPPC-04 were evaluated on how many trials reached the goal per problem and the average number of actions selected when successful. These planning problems were however relatively easy to solve as they did not account for \emph{natural dynamics} and/or large branching factors that occur in more realistic systems \cite{kolobov2012glutton}. FF-Replan \cite{FFReplan}, as specified in Section \ref{sec:planners} kindly remembered the committee of the IPPC-04 that the specified planning problems that contained no dead-ends were fairly easy to solve using relatively simple tools to handle with uncertainty.

\subsubsection{MDPs with Finite Horizon. }IPPC-11 decided to respond by introducing more realistic MDPs and their evaluation criteria by using MDPs with finite horizons. The planning problems are defined as tuples of the form \\ $\mathit{M\textsubscript{s0}(H) = \langle \langle S, A, T, R, s\textsubscript{0} \rangle , H \rangle }$ where $\mathit{S}$ s a finite set of states, $\mathit{A}$ a finite set of actions, $\mathit{T}$ is the transition function, similar to $\mathit{Pr}$ in the goal oriented MDP. $\mathit{R}$ represents the transitioning reward, $\mathit{s\textsubscript{0}}$ and $\mathit{H}$ is the number of decision epochs after which the process stops. Solving $\mathit{M(H)}$ means finding a policy for executing recommended state from the initial state over $\mathit{H}$ decision epochs \cite{kolobov2012gourmand}.

Translating finite horizon MDPs to goal-oriented domains results ends up in all state $H$ steps away from $s\textsubscript{0}$ are goal states. The evaluation criteria in finite horizon MDPs is the \emph{expected reward} and not the probability of the policy reaching the goal states.

\subsection{Planning algorithms}
\subsubsection{Replanning}
A replanner is a planner that takes a problem as input, plans a state-action policy, executes it online and whenever it encounters a state that was not in its state-action policy, it takes the current state as starting state and plans again until it has reached a goal or dead-end state. For our paper have chosen to evaluate FF-Replan \cite{FFReplan}, the winner of IPPC-04 and unenrolled winner of IPPC-06 as an inspiring, fairly simple planner that in our opinion is capable of employment in real-world domains.

\subsubsection{Probabilistic planning}
Probabilistic planners are used to solve probabilistic planning problems and should be able to account for uncertainty as opposed to classical planners. For our evaluation we have chosen to compare FF-Replan to a state-of-the-art probabilistic planner: \textsc{Gourmand}. \textsc{Gourmand} is the online successor of the \textsc{Glutton} plan, a Labelled Real-Time Dynamic Programming (LRTDP) based planner and the runner up of IPPC-11.

\subsection{Probabilistic Interestingness}
After IPPC-06, a baseline test for \emph{probabilistically interesting} planning domains was proposed by Little and Thi\'ebauw \cite{little2007probvsreplan}.  Problems that fail this test were deemed to be classical planning problems in disguise, favouring the use of replanners. A probabilistic planning problem is considered to be probabilistically interesting if and only if it has all the following structural properties \cite{little2007probvsreplan}:
% reference little & thiebauw again. Maybe also shortly define what all o, n, act etc. are.

\begin{itemize}
	\item there are multiple goal trajectories;
	\item there is at least on pair of distinct goal trajectories, $\tau$ and $\tau$ ', that share a common sequence of outcomes for the first n-1 outcomes, and where $\tau$\textsubscript{n} and $\tau$'\textsubscript{n} are distinct outcomes of the same action; and
	\item there exist two distinct goal trajectories $\tau$ and $\tau$' and outcome o $\in$ $\tau$ and o' $\in$ $\tau$' of two distinct actions a = act(o) and a'= act(o') such that executing a strictly decreases the maximum probability of reaching a state where a' can be executed.
\end{itemize}

%Following existing domains were said to be probabilistically interesting:
%
%\begin{itemize}
%	\item Drive
%	\item Exploding blocksworld
%	\item Pitchcatch
%	\item Schedule
%	\item Tireworld
%\end{itemize}


%---------------------------------------------------------------------

\section{Planners}
\label{sec:planners}

FF-Replan \cite{FFReplan}, a replanner based on determinization techniques and a classical planner, surprisingly outperformed other participating MDP solvers in the IPPC-04 and by doing so changed the course of the planning competition entirely. By relying on search and removing all probabilistic information from the problem, FF-replan functions with enormous speed and efficiency. This method has proven to outperform MDP solvers, the state-of-the-art probabilistic planners for uncertainty of that time. Since then, Little et al. has argued for \emph{probabilistically interesting} domains. By introducing finite horizon MDPs in IPPC-11, state of the art planners have been adapted to cope with large branching factors to approximate infinite planning problems. Because these ``planning problems'' can often be solved using a smaller horizon $h < H$ as introduced in Section \ref{sec:background}, they represent optimization problems rather than planning problems, favoring current state of the art planners over replanners. This section introduces the shift of the planners towards optimization  by discussing FF-replan and a current state of the art probabilistic planner \textsc{Gourmand} and its predecessors.

\myworries{misschien moet dat laatste zinnetje ergens anders?}

\subsection{FF-replan}
\label{sec:ffreplan}
 FF-Replan generates a deterministic planning problem by determinizing the input domain at the start of each planning iteration. This classical deterministic planning problem allows for the use of the classical FF planner \cite{Hoffmann01theff}. The FF planner then returns an ordered state-action plan for the deterministic problem of the current state. This plan is executed until an unexpected state is encountered in the following manner:

\begin{enumerate}
	\item Start planning at $s_0$
	\item Determinizes the input domain
	\item Use FF on the classical planning problem
	\item Executes ordered state-action plan until
	\item Unexpected state is encountered or
	\begin{itemize}
		\item Start planning at $s_t$ at time t
		\item Repeat step 2 - 6
	\end{itemize}
	\item Goal state or dead-end is reached
\end{enumerate}

FF-Replan has some obvious shortcomings. It removes all probabilistic information from the problem when determinizing the domain. The success of the planner is therefore fully reliable on the chosen determinization heuristic. As FF-replan uses no roll-out policy or future simulation, it has no way of avoiding \emph{avoidable} dead-ends. This explains why FF-replan performs poorly in domains containing dead-ends.

For a planner to be successful in finite-horizon MDPs it needs to be scalable and sensitive to an expected reward maximization, the evaluation criteria of these domains. To cope with these domains, trial based methods such as \emph{Labelled Real-Time Dynamic Programmers} (LRTDPs) have been introduced and optimized to find an optimal policy in a dynamic programming-based way.

%----------------------------------------------------------------------------------------

\subsection{LRTDP algorithms}

The LRTDP algorithm tries to reach the goal using a series of trials starting from $s_0$ and is specifically convenient for only memorizing states reachable from $s_0$ and therefore uses and needs less memory space in the following manner:

\begin{enumerate}
	\item Start trial at $s_0$
	\item Take action based on current value function $V$
	\item Perform Bellman backup
	\item Sample the outcome and update $V$
	\item Transition to the next state
	\item Repeat step 2 - 5 until a goal, dead end or converged state is reached
	\item Check if all states have converged, otherwise start a new trial (step 2 - 6)
\end{enumerate}

Adapting LRTDP to a finite horizon MDP $M(H)$ solver \emph{LRTDP-FH} requires for each trial to start at $s_0$ or $M(s_0, H)$ and run for at most $H$ time steps. All states $H$ steps away from $s_0$ would translate to goal states in a goal-oriented MDP. \\

%However, it's anytime performance to meet the demands of a dynamic environment is not very good, creating the need for a more sophisticated approach \cite{kolobov2012glutton}.
The \emph{Labeled Reverse Real-Time Dynamic Programmer} (LR$^2$TDP) uses \emph{reverse iterative deepening} and LRTDP-FH to incrementally solve a sequence of MDPs $M(1)$, $M(2)$, \ldots , $M(H)$ by relying on the fact that a successful policy can be produced by solving a MDP-FH $M(h)$ for a smaller horizon $h < H$.
%LR$^2$TDP\@ iterates on the distance from the horizon and allows for reusing precomputed state-action plans from converged states.

\begin{enumerate}
	\item Run LRTDP-FH to solve $M(1)$ or $M(s_0, 1)$
	\item Run LRTDP-FH to solve $M(h)$ or $M(s_0, h)$ where $h < H$ by backchaining from the horizon $H$
	\item Repeat step 2 until $H$ is reached or time has run out
\end{enumerate}
By iterating over the distance from the horizon the state values computed when solving $M(1)$, $M(2)$, \ldots, $M(h-1)$ can be reused. Solving $M(h)$ thus only takes an incremental effort over the solution of $M(h-1)$. If stopped prematurely  (LR$^2$TDP) is likely to provide a better policy as for many MDPs the optimal policy $\tau^*$ is optimal or near-optimal for $M(H)$. By itself,  (LR$^2$TDP) would however not perform well in MDPs with large branching factors as computing the Bellman backups would become too expensive carried out over a large fraction of the state space.

\subsection{\textsc{Glutton}}
The \textsc{Glutton} planner enhances LR$^2$TDP to cope with large branching factors by using several optimization techniques. It employs LR$^2$TDP in an off-line fashion by initializing the algorithm with the \emph{Max-Reward} heuristic that places an upper bound on the optimal value function $V^*$. It optimizes for large branching factors by performing the transition function in a subset of transition states $s'$. It distinguished between \emph{exogenous} effects caused by natural dynamics and \emph{pure} effects caused by the action itself and a mixed of both. It also utilized the saved memory space caused by mapping only the converged states in the hash-table by using the free space as cache for storing successor states computed using the transition function.
Since \textsc{Glutton} subsamples the transition function, it may not know a good action to perform in states it missed due to subsampling. To compensate for these states, \textsc{Glutton} has to resort to a \emph{default policy}. \todo{Wel of niet de default policy uitleggen}.
%---------------------------------------------------------------------
%During IPPC-11 1 out of 5 planners used a LRTDP-based system and managed to outperform three other planners using the dominant algorithm for probabilistic planning: UCT. This caused to wonder whether LRTDP planners could outperform UCT based systems.  UCT forces an expert to guess a non-shortsighted lookahead value for which it should be able to converge on encountered states. Choosing this value can have very bad consequences and its existence causes the UCT to require some input, taking away from its autonomicity. At the IPPC-11 \textsc{Glutton} was an offline LRTDP based planner that came second. As PROST employed UCT in an online manner, \textsc{Gourmand} is an online adaptation of the \textsc{Glutton} algorithm to test whether LRTDP performs better if it is employed online.
%
%UCT picks an action based on it current quality estimate and an \emph{exploration term} which forces the planner to choose actions that have rarely been tried in the past. This makes UCT suitable for a wider scale of scenarios but also implies an unreliable termination condition.

When however dealing with large branching factors, offline planning is often a waste of time and memory space since there is a vast amount of states the planner may never reach in a lifetime. Such problems are far better solved using online planners that find an optimal policy $\tau^*$ or an approximation of this policy for the \emph{current} state.
%This is done by choosing a certain lookahead value $L$ and solving the MDP $M_s(L)$ to solve $M_0(H)$ where L $<$ H and then select $\pi^*_L(s,L)$ using the evaluation criteria. This lookahead value is determined using the per-process time constraint.

\subsection{\textsc{Gourmand}}
The \textsc{Gourmand} is an online adaptation of the \textsc{Glutton} planner and ensures a good \emph{anytime performance} using a lookahead value $L$ to approximate a near-optimal policy for $M(H)$ where $L < H$ and is dynamically chosen. At every iteration over the distance to the horizon $H$, it divides the total remaining time $\overline{T}$ equally amongst the remaining iterations and virtually allocates time: $T_t = \frac{\overline{T}}{H - t}$. During the first epoch, or iteration it estimates how long solving a state takes for different lookahead values. Using this estimate and a virtual time allocation \textsc{Gourmand} determines the largest lookahead value $L_t$ for which it can almost certainly solve the current epoch  and whether it could solve for a larger lookahead $\widehat{L}_t$ by taking away time from the next epoch, still guaranteeing that it can solve both epochs. If so, it solves the current epoch for $\widehat{L}_t$ and otherwise $L_t$. This way \textsc{Gourmand} never has to resort to default policies and converges relatively fast to a good policy in an online fashion. The \textsc{Gourmand} planner has proven to outperform the current state of the art planners utilizing its robust any-time performance \cite{kolobov2012gourmand} in the following manner:

\begin{enumerate}
	\item Start \textsc{Gourmand} at $s_0$
	\item Initialization of first epoch
	\begin{enumerate}
		\item Virtually allocate time for  $T_t = \frac{\overline{T}}{H}$
		\item Run LR$^2$TDP($M_s(H), T_t$)
		\item Adjust running average $\overline{T}$
	\end{enumerate}
	\item Virtually allocate time  $T_t = \frac{\overline{T}}{H - t}$
	\item Check if lookahead can be increased $\widehat{L}_t = L_t + 1$ else $\widehat{L}_t = L_t $
	\item Run LR$^2$TDP($M_s(\widehat{L}), \widehat{T}_t$)
	\item Adjust running average $\overline{T}$
	\item Determine action based on (near) optimal $\tau^*$
	\item Transition to next state
	\item Repeat steps 3 - 8 until time has run out
\end{enumerate}

\todo{write some kind of conclusion}

%---------------------------------------------------------------------

\section{Domains}
Following the theoretical comparison of probabilistic planning and replanning techniques by Little and Thi\'ebauw, a baseline test for \emph{probabilistically interesting} domains is proposed. Problems that fail this test were deemed to be classical planning problems in disguise, favouring the use of replanners \cite{little2007probvsreplan}. This section describes interesting domains in which either FF-Replan or \textsc{Gourmand} prevailed during the International Probabilistic Planning Competitions. These domains are evaluated for how well they portray real-world scenarios. The domains we deem most interesting for the purpose of this paper are adjusted to match their real-world scenario. Little et al.'s baseline test helps us to predict which planner would perform well on these domains. We will then conclude our evaluation by comparing these results to the result of our experiments described in Section~\ref{sec:experiment}.

\subsection{IPPC-11 Domains}

The competitor FF-Replan \cite{FFReplan} kindly remembered the committee of the IPPC-04 that problems containing no dead ends and action costs are fairly easy to solve without much handling of uncertainty. The baseline test of Little et al.\@ evaluated the competition problems of the IPPC described in PPDDL to correct the possible inaccurate perception of replanners vs.\@ probabilistic planners. IPPC-11 responded by introducing more realistic MDPs and evaluation criteria: finite-horizon MDPs evaluated on the expected reward of the created policies.

\textsc{Gourmand} vastly outperformed IPPC-11 UCT-based probabilistic track winner PROST by far on goal-oriented domains Navigation and Crossing Traffic \cite{kolobov2012gourmand}. PROST however mildly outperforms \textsc{Gourmand} on the Game of Life, SysAdmin and the Traffic domain as UCT does not need to perform expensive Bellman Backups on these domains that include large branching factors.

\subsubsection{Navigation}

The navigation domain represents a grid in which a robot must reach a goal. Every cell offers the robot a different chance of disappearing. The robot needs to choose a path which gets it to the goal most reliably within the finite horizon time \cite{sanner2011navigation}. Because \textsc{Gourmand} uses a dynamic lookahead, it performs quite well in this domain.

%ADD IMAGE HERE

\subsubsection{Traffic}
This domain represents a simple binary version of the cell transition model (CTM) for modelling traffic based on the original CTM Tech Report \cite{sanner2011traffic}. Because of the binary variables and no intermediate variable restrictions, this model is simplified and ignores traffic aspects such as turns and turn probabilities. This model also uses concurrent actions and the total actions has an upper bound of $2^{\#~of~intersections}$.

%ADD IMAGE HERE

\subsection{Elevators}

In the elevators domain there are a number of elevators delivering passengers
to either the top or bottom floor. Potential passengers arrive at any floor at
a certain rate (depending on which floor). The elevators can only move in the
direction it is indicating, so passengers know whether to board or not. The
goal of the elevators is to transport as many as possible passengers to their
destinations.

\subsection{SysAdmin}

The SysAdmin domain describes a network of connected servers. Each server has
a probability that it shuts down, and if is not running, it has a probability
to reboot. The probability that a server shuts down depends on the number of
running connected servers.

%\subsection{IPPC-04 Domains}
%Duing IPPC-04, the domains were written in the Probabilistic PLanning Domain Description Language (PPDDL). 30 trials per problem were to be solved in 15 minutes.
%
%\subsubsection{}
%
%\subsubsection{}

\subsection{Real-World Domain Evaluation}

\subsubsection{Navigation}

In the navigation problem there is a chance that the robot disappears after
entering a cell. The disappearing action is a dead end. Disappearing here is a
metaphor for the robot breaking down, or that it disappears from the radar with
the base station. In many of such cases it's a big problem, but can be repaired
with a high cost. In the navigation domain a step to another cell has a cost of
1. If instead of a dead end the cost is $n$ (e.g. 5) instead of 1, which
represents the cost of repairing the robot, dead ends are removed. Without dead
ends the problem becomes probabilistic uninteresting, which makes it suitable for
replanning planners.

\subsubsection{Traffic}

The traffic domain describes a crossroad with traffic lights, modelled as
cells. Either the north to south road has a green light, the west to east road
or all lights are red. The model uses a finite state machine that handles the
lights so that the lights are never green at the same time. This would be the
only dangerous situation on the crossroad. The difficulty in this domain is
the number of states, which grows exponentially with the number of cells.
Besides that it is an uninteresting probabilistic problem because it does
not satisfy the probabilistic interesting test.

\subsubsection{Elevators}



\subsection{Baseline test for Probabilistic Interesting domains}

%---------------------------------------------------------------------

\section{Future Experiment}
\label{sec:experiment}

Experiments can empirically evaluate the performance of the adjusted domains.

\subsection{Setup}

The planning problems are taken from the IPPC-11 suite and are written in RDDL,
which are publicity available. A domain consists of the domain description and
instances of a domain problem. The domain description is read by RDDLSim, which
is a server a planner can communicate with, and simulates the real world after
the planner sends an action.

\subsection{FF-Replan with Finite Horizon}

\textsc{Gourmand} already works with RDDLSim and MDPs with finite horizon. The original
FF-Replan is a Goal-Oriented MDP planner. For a Finite-Horizon each state at
the horizon is a goal. To compare \textsc{Gourmand}, that works with MDP-FH, with
FF-Replan, FF-Replan needs a slight update. It should not only execute FF for
one goal, but for all goals at the horizon. It could then start executing the
plan with the highest reward. Depending on the determinization method,
\emph{single-outcomes} or \emph{all-outcomes}, not all goals might be
reachable. Of course the performance decreases with an increase of goal states
at the horizon. A hash-table cache, as FF-Replan already uses can still be used
to store plans generated by FF from a state to goal states, however not only
the plan should be stored, but also the reward for this plan should be stored
to pick the plan with the highest expected reward.

Using the framework of \textsc{Glutton} and \textsc{Gourmand}, and the FF heuristic found in mGPT
\cite{bonet2011mgpt}, the updated FF-Replan can be implemented. The first step
of \textsc{Gourmand} is parsing the problem and creating a structure of the problem.
Instead of executing the \textsc{Gourmand} algorithm, this structure can be used to
determinize the problem by setting all probabilities of each action to 1
(\emph{all-outcome}) or all but one to 0 (\emph{single-outcome}). Using the
FF heuristic it can find plans for each goal and calculate the expected reward.
Then it can reuse the \textsc{Gourmand} framework to execute the plan by sending the
action to the RDDLSim server. Like FF-Replan if the server returns with a state
that is not in the plan the algorithm will replan. If a goal is reached it will
output the achieved reward.

\subsection{Hypothesis}

%---------------------------------------------------------------------

\section{Related Work}

In sections above we have described the GOURMAND planner and FF-Replan as two
probabilistic planners of the IPPC competition.
RFF~\cite{teichteil2010incremental} is an offline planner that generates a
policy: for each reachable state it determines a plan to the goal. Like
FF-Replan it uses determinization and FF\. FF returns an initial plan from the
initial state to a goal state. For each state in the plan the successor states
are added to the structure, after which Monte-Carlo sampling is used. States
where the probability that they're reached is too low (below a threshold) are
removed. The successor states are used as initial states in the next step. If
FF fails to find a plan to the goal from this new state, the state is removed
from the structure too. Results showed that RFF outperformed other competitors.
A higher threshold means that RFF will generate more policies that are prone to
replanning.

\subsubsection{PROST}

\subsubsection{FF-H+}

%\subsection{Probabilistic Planning via Determinization in Hindsight (2008)~\cite{yoon2008probabilistic}}
%
%Hindsight optimization is an online technique that evaluates one-step-reachable
%states by sampling future outcomes to generate multiple non-stationary planning
%problems which are deterministic and can be used using search. It re-interprets
%FF-Replan's~\cite{FFReplan} strategy randomly generating a set of non-stationary
%determinized problems and combining their solutions.

%\subsection{Improving Determinization in Hindsight for Online Probabilistic Planning (2010)~\cite{yoon2010improving}}
%Hindsight optimization has enjoyed some success for online probabilistic
%planning. Although it has proven to be effective for many domains, it is
%computationally expensive. This paper presents significant improvements.  1) a
%method for detecting potential useful actions, 2) exploit determinism in the
%domain by reusing relevant plans, 2) improves action evaluation by increasing
%the chance that at least one deterministic plan reaches a goal.

%\subsection{Probabilistic Planning in the Graphplan Framework (1999)~\cite{Blum99probabilisticplanning}}
%% might not be necessary?
%Graphplan is a successful planning algorithm for classical STRIPS domains. This
%paper explores the extend it can be used in \emph{probabilistic} domains. The
%paper discusses two variations of Graphplan: PGraphplan and TGraphplan.
%PGraphplan produces an optimal plan, while TGraphplan produces a sub-optimal
%plan but it has increased speed. By comparing the speed and quality of the
%two planners the authors are able to estimate how far they are from the ideal.

%\subsection{Compiling Conformant Probabilistic Planning Problems into Classical Planning (2013)~\cite{taig2013conformant}}
% not necessary?

%\subsection{How Much Does a Household Robot Need To Know In Order To Tidy Up? (2013)~\cite{nebel2013much}}
%
%For household robot planning, it appears easy, but actually involves
%uncertainty. For tidying things up objects might not be there, or sensing
%operations might tell things wrong. This paper looks into conditions for
%classical planning in a replanning loop in order to solve nondeterministic
%partially observable open domains This paper looks into conditions for
%classical planning in a replanning loop in order to solve nondeterministic
%partially observable open domains.

%\subsection{Progressive heuristic search for probabilistic planning based on interaction estimates (2013)~\cite{martin2013progressive}}
%
%In this paper a probabilistic plan graph heuristic is described which computes
%information about the interaction between actions and between propositions.
%This information is used to find better relaxed plans to compute the
%probability of success. This information guides a forward state space search
%for high probability, non-branching seed plans. These plans are then used in
%a planning and scheduling system that handles unexpected outcomes by runtime
%replanning.

%\subsection{Translation based approaches to probabilistic planning (2013)~\cite{taig2013translation}}
% I don't think one is valuable



\subsection{Probabilistic planning vs Replanning}
Inspired by the FF-Replan entrance in the IPPC competition, this paper described a probabilistic interestingness quality of domains. It then compares probabilistic planners with replanners in the different domains to draw conclusions. Classical planning with replanning upon failure when there is so little or so much uncertainty about the world that it is not worth modelling and reasoning about or when goals dynamically change or when the problem is too large to solve at once. Probabilistic planners are more appropriate when avoiding irreparable or costly failures. Probabilistic planning computes a plan that can handle all foreseeable contingencies and is preferable for quick reactions in closed control loops.

The following structural properties are identified for comparing probabilistic planning with replanning:
\begin{itemize}
	\item `dead' end states
	\item the degree to which the probability of reaching a dead end state can be reduced through choice of actions
	\item number of distinct trajectories
	\item presence of mutual exclusion
\end{itemize}

If there are no dead ends, a replanner will always succeed. It is impossible to improve on the unavoidable dead ends (positive probability of reaching when executing which cannot be reduced without reducing the probability of reaching the goal).

% repetition of what's already stated earlier

A probabilistic planning problem is considered to be probabilistically interesting if and only if it has all the following structural properties:

\begin{itemize}
	\item there are multiple goal trajectories;
	\item there is at least on pair of distinct goal trajectories, $\tau$ and $\tau$ ', that share a common sequence of outcomes for the first n-1 outcomes, and where $\tau$\textsubscript{n} and $\tau$'\textsubscript{n} are distinct outcomes of the same action; and
	\item there exist two distinct goal trajectories $\tau$ and $\tau$' and outcome o $\in$ $\tau$ and o' $\in$ $\tau$' of two distinct actions a = act(o) and a'= act(o') such that executing a strictly decreases the maximum probability of reaching a state where a' can be executed.
\end{itemize}

Problems that fail to be \emph{probabilistically interesting} are actually classical planning problems in disguise and for those replanners are more appropriate. A replanner can also perform optimally for probabilistically interesting planning problems whenever the `most promising' goal trajectory is the correct thing to do. They create several domains to explore the issue of replanning vs\@. probabilistic planning.

Following existing domains were said to be probabilistically interesting:
\begin{itemize}
	\item Drive
	\item Exploding blocksworld
	\item Pitchcatch
	\item Schedule
	\item Tireworld
\end{itemize}

They conclude by stating that the negative perception of probabilistic planning by the ICAPS community was inaccurate. They state that a synthesis of planning and replanning techniques could make a larger number of probabilistic planning problems practicably solvable than is currently possible.



%---------------------------------------------------------------------

\section{Conclusion}

\begin{itemize}
	\item Conclude the work done in the paper
	\item Hint to some future work developments
\end{itemize}

%\section{Some papers}
%
%\begin{itemize}
%	\item \cite{Hoffmann01theff}
%	\item \cite{FFReplan}
%	\item \cite{teichteil2010incremental}
%	\item \cite{yoon2008probabilistic}
%	\item \cite{teichteil2012fast}
%	\item \cite{bonet2011planning}
%	\item \cite{taig2013translation}
%	\item \cite{martin2013progressive}
%	\item \cite{taig2013conformant}
%	\item \cite{2013workshop}
%	\item \cite{nebel2013much}
%	\item \cite{little2007probvsreplan}
%	\item \cite{keller2012prost}
%	\item \cite{kolobov2012glutton}
%	\item \cite{sanner2008rddl}
%	\item \cite{kolobov2012gourmand}
%\end{itemize}


\bibliographystyle{plain}
\bibliography{paper}

\end{document}
