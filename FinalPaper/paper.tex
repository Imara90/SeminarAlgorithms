\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{fixltx2e}

\usepackage{url}
\urldef{\mailsa}\path|{i.c.t.m.speek, a.c.stolwijk}@student.tudelft.nl|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip%
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter% start of an individual contribution

% first the title is needed
\title{Validating FF-Replan vs.\@ a State-Of-The-Art Probabilistic Planner in Translated Real World Domains}

% a short form should be given in case it is too long for the running head
\titlerunning{FF-Replan vs Probabilistic Planner in Real World Domains}

\author{I.C.T.M Speek, A.C. Stolwijk}

%
\authorrunning{Validating Planners in a Real World Domain}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Seminar Algorithms, Embedded Software,
MSc Embedded Systems,\\
Delft University of Technology\\
\mailsa\\
}

%\toctitle{Lecture Notes in Computer Science}
%\tocauthor{Authors' Instructions}

\maketitle

%--------------------------------------------------------------------

\begin{abstract}
	This paper argues that for many real life domains dead-ends are the end of
	the world, and by for example human interaction the goal can still be
	reached. By relaxing the definition of a dead-end, we argue that for many
	real life domains a simpler planning algorithm, like FF-Replan, can be
	sufficient, and that it's not needed to have newer and sophisticated
	probabilistic planners.
\end{abstract}

%--------------------------------------------------------------------

\section{Introduction}

Since the first the International Probabilistic Planning Competition (IPPC),
problems have undergone radical changes. Dominating planners of previous
incorporations had a strong focus on goal-oriented MDPs, determinizing and
using powerful classical planners (e.g. FF \cite{Hoffmann01theff}) and
replanning. New benchmark problems have demonstrated weaknesses in existing
solution techniques.

Little and Thi\'ebaux have developed a baseline test in
\cite{little2007probvsreplan} to evaluate the problems of the fifth International
Planning Competition (IPC-5). This test determines if a planning problem is
`probabilistically interesting'. Problems that fail this test are classical
planning problems in disguise. Knowing this it is easy to design a problem
where a replanner performs badly or incredibly well.

The IPPC-11 introduced Relation Dynamic Influence Diagram Language (RDDL),
which can design domains that are more probabilistically interesting. The top
performers were PROST \cite{keller2012prost} and Glutton
\cite{kolobov2012glutton}.

While it is interesting to make problems harder and invent better planning
algorithms to solve those probabilistic complex problems, we evaluate whether
or not these problems are coherent with real-world situations.

The main contribution of this paper is an evaluation of FF-Replan versus a
probabilistically state of the art planner GOURMAND in the realistically
augmented domains of IPPC-11. If these domains fail the baseline test proposed
by Little et\@. all, they would translate to classical planning problems in
disguise and could be easily solved by FF-Replan.

\begin{itemize}
	\item Describe probabilistic planning problem
	\item Introduce planning for uncertainty
	\item Describe how deterministic classical planners can be used in probabilistic domains and motivate why this is interesting
	\item Introduce the qualities of replanning and the second planning algorithm
	\item Describe the need for a new domain for real world applications
	\item Describe the goal and tools used to reach this goal
\end{itemize}

%---------------------------------------------------------------------

\section{Problem Formulation}

\subsection{Markov Decision Process}

\subsubsection{Goal-oriented MDPs}

\subsubsection{MDP with Finite Horizon}

\subsection{Planners}

\subsubsection{Classical Planners}

A planner tries to find a sequence of actions such that a goal or maximum
reward is achieved. A classical planning problem consists out of states,
actions, conditions and a goal state.

\subsubsection{Probabilistic Planning} Classical Planners expect a
deterministic world. For many Real World applications actions happen with a
certain probability. This kind of Probabilistic Planning Problems can be
described with Markov Decision Processes (MDPs). For the purpose of our
analysis we define the problem as:

\begin{itemize}
	\item finite set of states $S$
	\item an initial state $s_o \in S$
	\item a $G \subseteq S$ of (absorbing) goal states
	\item a set $O$ of outcomes; probability of $o \in O$ is $\Pr(o)$
	\item a (total) deterministic transition function $T(o,s) \in S$ for all
		outcomes $o \in O$ and states $s \in S$
	\item sets $A(s)$ of applicable actions for each $s \in S$, together with
		function $out(a) \subseteq O$ mapping each action to a set of outcomes
		such that each outcome $o \in O$ belongs exactly to one action $act(o)$
		and $\sum_{o \in out(a)} \Pr(o) = 1$ for all $a$
\end{itemize}

% depends if we should include this, what we tell about GOURMAND / RDDL...

Besides a single or multiple goals, some planning problems also include a
reward function: For every $s \in S$, $s' \in S$, and $a \in A(s)$,
$R(s,a,s')$ is the reward function. The goal if the problem is to maximize
the reward until a goal is reached. If a Finite Horizon (number of steps of the
plan) is used, the goal of the problem is to achieve the highest reward within
the horizon.

A probabilistic planner takes the probabilistic planning problem as input and
builds and outputs a plan for the problem.

In this paper we use GOURMAND as a representative probabilistic planner, as
it's the improved version of Glutton, the runner-up of the IPPC-11 competition.

\subsubsection{Replanning}

An Replanning Planner takes a probabilistic problem and tries to solve it with
replanning. First the problem is determinized, a compilation from a
probabilistic problem to a deterministic problem. This deterministic problem
be solved by a classical planner which returns a plan. The Replanner then
executes the plan. If it ends up in a state which is different from the plan,
it executes the classical planner to get a new plan. This is repeated until a
goal is reached.

In this paper we use FF-Replan as a representative replanning planner.

\begin{itemize}
	\item Summarize shortcomings of the previous solutions
	\item Describe the planning problem
	\item Argument the choice for 2 current papers to analyze
	\item Define how we would like to test the planning algorithms in a domain by our choice
\end{itemize}


\subsection{Planning problem}

\begin{itemize}
	\item Describe the MDP finite horizon planning problem (useful for modelling nearly infinite processes)
	\item reason about the \emph{augmented state space} of M(H), which is a set \emph{S x (0, ... , H)} of state number of steps-to-go pairs
	\item Describe reward heuristic
	\item Solving M(H) means finding a policy: a rule for selecting actions in augmented states so that executing the actions recommended by the policy starting in the initial state will accumulate the largest expected reward over H steps time
	\item The challenge or large branching factors (induced by natural dynamic)
\end{itemize}

\subsection{Probabilistic Interestingness}

Following the theoretical comparison of probabilistic planning and replanning techniques by Little and Thi\'ebauw, a baseline test for \emph{probabilistically interesting} domains is proposed. Problems that fail this test were deemed to be classical planning problems in disguise, favouring using replanners\cite{little2007probvsreplan}. This section describes interesting domains in which either FF-Replan or GOURMAND prevailed during the International Probabilistic Planning Compeitions. We will then translate the most interesting domains to a complete "real-world" domain and evaluate them using the probabilistically interesting baseline test. This paper concludes on evaluation both planners using the translated domains.

The competitor FF-Replan\cite{FFReplan} remembered the committee of the IPPC in 2004 that problems containing no dead ends and action costs are fairly easy to solve without much handling of uncertainty. The baseline test evaluates the competition problems of the IPPC described in PPDDL to correct the possible inaccurate perception of replanners vs.\@ probabilistic planners.

Following existing domains were said to be probabilistically interesting:

\begin{itemize}
	\item Drive
	\item Exploding blocksworld
	\item Pitchcatch
	\item Schedule
	\item Tireworld
\end{itemize}

A probabilistic planning problem is considered to be probabilistically interesting if and only if it has all the following structural properties:

% reference little & thiebauw again. Maybe also shortly define what all o, n, act etc. are.

\begin{itemize}
	\item there are multiple goal trajectories;
	\item there is at least on pair of distinct goal trajectories, $\tau$ and $\tau$ ', that share a common sequence of outcomes for the first n-1 outcomes, and where $\tau$\textsubscript{n} and $\tau$'\textsubscript{n} are distinct outcomes of the same action; and
	\item there exist two distinct goal trajectories $\tau$ and $\tau$' and outcome o $\in$ $\tau$ and o' $\in$ $\tau$' of two distinct actions a = act(o) and a'= act(o') such that executing a strictly decreases the maximum probability of reaching a state where a' can be executed.
\end{itemize}

%---------------------------------------------------------------------

\section{Planners}

\subsection{FF-Replan}

\subsection{GOURMAND}

The IPPC-2011 introduces more realistic MDPs to accommodate real-world systems with large branching factors cause by \emph{natural dynamics} and a limited time for the planner to come up with a policy. For this reason planners needed to be scalable and sensitive to the expected reward maximization. Glutton introduces an off-line algorithm based on LR\textsuperscript{2}TDP and relies on the fact that a successful policy can be produced by solving an MDP $M(H)$ for a smaller horizon $h$. Incrementally solving the sequence $M(1)$, $M(2)$ forms the basis of LR\textsuperscript{2}TDP\@. By iterating on the distance from the horizon, data can be reused (and is stored in the cache until the hash-table is full). The max-reward heuristic finds the largest horizon value $h'$ for which already a heuristic value $V$ is estimated. The  computation time remains much lower because it runs quickly into states that converged while solving $M(h-1)$. The max-reward computation relies on knowing the maximum $R_{max}$ any action can yield in any state or an upper bound on it.

\subsubsection{LRTDP}
Using goal oriented MDPs, the \emph{Labelled Real-Time Dynamic Programmer} (LRTDP) operates a series of trials starting from $s_0$.

\begin{itemize}
	\item Trials start at $s_0$
	\item Choose best action greedily in the current state based on current $V$
	\item Perform  Bellman backup on the current state
	\item Sample the outcome as the chosen action
	\item Transition to the new state
	\item Repeat until a goal, dead end or converged state is reached
	\item After trial, special convergence check on all states in the trial is performed to prove the convergence of these values
\end{itemize}

\subsubsection{LRTDP-FH}

Adapting LRTDP to a finite horizon MDP \emph{M(H)} requires to let each trial start at ($s_0$) and run for at most H time steps. A steps $H$ steps away from $s_0$ would translate to goals in a goal-oriented MDP. However, it's anytime performance to meet the demands of a dynamic environment is not very good, creating the need for a more sophisticated approach \cite{kolobov2012glutton}.

\subsubsection{LR\textsuperscript{2}TDP}

LR\textsuperscript{2}TDP uses \emph{reverse iterative deepening} and LRTDP-FH to solve a sequence of MDPs $M(1)$, $M(2)$, \ldots , $M(H)$, in that order. It first decides how to act optimally in ($s_0$, 1), equivalent to solving M(1) , then ($s_0$, 2), equivalent to solving $M(2)$, eventually solving $M(H)$ or running out of time halting a $M(h')$ where $h' < H$. This incremental effort allows for reusing state values computed in previous states, so it memorizes all values and solves for smaller horizons by backchaining from the goal LRTDP-FH iterates over search depth while LR\textsuperscript{2}TDP iterates over the distance from the horizon and is likely to be better because:

\begin{itemize}
	\item In many MDPs, the optimal policy $M(h)$ for some $h < H$ is near optimal for M(H), it thus may have a solution when stopped prematurely
	\item it's trials are very short because backchaining from the goal leads to a lots of convergence
	\item It optimally memorizes a large fraction of augmented states to save memory
	\item a large fraction of state space may be unreachable from $s_0$ and it avoid storing information it does not need
	\item it should however use a heuristic to treat states that has never been encountered when stopped prematurely, could be a fall back or a default action.
\end{itemize}

\subsubsection{GLUTTON}

Glutton uses the \emph{Max-Reward} heuristic that relies on knowing the maximum reward $R_{max}$ any action can yield in a state or an upper bound on it. Large branching factors induce that the computation of the Bellman Backups becomes prohibitively expensive. Glutton uses the following mechanisms to optimize for large branching factors:

\begin{itemize}
	\item Subsampling the transition function, is not enough to decrease the amount of computations yet
	\item Separating out Natural Dynamics: to reuse variable samples when generator successors for multiple actions, by separating the actions' effects into natural dynamics (\emph{exogenous}), those due to the action itself (\emph{pure effects}) and those do a mix of both.
	\item Caching the transition function samples: as LR\textsuperscript{2}TDP populates the state-value hash table lazily, Glutton uses  this memory as a cache for samples from the transition function, so it doesn't have to regenerate the action when encountering the same $(s,h)$ pair. When full, it chooses state-action pars for eviction ans replacement randomly.
	\item Default policies: Glutton uses 3 tricks to determine a policy when stopped prematurely. It compares the optimal action from a $h'$ with a simple cyclic policy or a complete random policy and chooses the best of these based on the reward.
\end{itemize}

During IPPC-2011 1 out of 5 planners used a LRTDP-based system and managed to outperform three other planners using the dominant algorithm for probabilistic planning: UCT. This caused to wonder whether LRTDP planners could outperform UCT based systems.  UCT forces an expert to guess a non-shortsighted lookahead value for which it should be able to converge on encountered states. Choosing this value can have very bad consequences and its existence causes the UCT to require some input, taking away from its autonomicity. At the IPPC-2011 GLUTTON was an offline LRTDP based planner that came second. As PROST employed UCT in an online manner, GOURMAND is an online adaptation of the GLUTTON algorithm to test whether LRTDP performs better if it is employed online.

UCT picks an action based on it current quality estimate and an \emph{exploration term} which forces the planner to choose actions that have rarely been tried in the past. This makes UCT suitable for a wider scale of scenarios but also implies an unreliable termination condition.

Because real world domains contain Natural Dynamics that introduce large branching factors, offline planning is often a waste of time and memory space since there is a vast amount of state the planner may never reach in a lifetime. Such problems are far better solved using online planners by finding a $\tau$* or an approximation for the current state, executing it and repeating. This is done by choosing a certain lookahead  L and solving MDP \emph{M\textsubscript{s}(L)} to solve \emph{M\textsubscript{0}(H)} where L $<$ H and then select \emph{$\pi$*\textsubscript{L}(s, L)} recommended by the optimal policy. This lookahead value is determined using the per-process time constraint.

\subsubsection{GOURMAND}
In contrast to GLUTTON, GOURMAND always makes an informed choice in any state where it ends up when the time T allocated for solving the MDP runs out because of the online fashion it uses LRTDP. GOURMAND first divides the time among all remaining epochs, determines the largest lookahead L for which it can almost certainly solve any remaining epoch if T is assigned.  It then checks if it can solve even for a larger lookahead L by taking away a small configuration time from future epochs  while still guaranteeing that it can solve for Lookahead L in each of them. This was GOURMAND adaptively picks a good lookahead value at each desicion epoch given the per-process constraint T. This can only be achieved by knowing how long it takes for LRTDP to solve a state for certain lookahead values. It uses LR\textsuperscript{2}TDP to collect this data. This version of LRTDP is more efficient because of the early convergences and it allows measuring how long on average the solving of a task takes. GOURMAND uses the same engineering optimizations to to handle evaluate actions, iterating over the successors of s under a by the Bellman Backups. GOURMAND eventually outperforms both GLUTTON and PROST, the winner of the IPPC-11\cite{keller2012prost}.


%---------------------------------------------------------------------

\section{Domain Descriptions}
Following the theoretical comparison of probabilistic planning and replanning techniques by Little and Thi\'ebauw, a baseline test for \emph{probabilistically interesting} domains is proposed. Problems that fail this test were deemed to be classical planning problems in disguise, favouring the use of replanners \cite{little2007probvsreplan}. This section describes interesting domains in which either FF-Replan or GOURMAND prevailed during the International Probabilistic Planning Compeitions. These domains are evaluated for how well they portray real-world scenarios. The domains we deem most interesting for the purpose of this paper are adjusted to match their real-world scenario. Little et all's baseline test helps us to predict which planner would perform well on these domains. We will then conclude our evaluation by comparing these results to the result of our experiments described in Section \ref{sec:experiment}.

\subsection{IPPC-2011 Domains}
The competitor FF-Replan \cite{FFReplan} kindly remembered the committee of the IPPC-04 that problems containing no dead ends and action costs are fairly easy to solve without much handling of uncertainty. The baseline test of Little et. all evaluated the competition problems of the IPPC described in PPDDL to correct the possible inaccurate perception of replanners vs.\@ probabilistic planners. IPPC-2011 responded by introducing more realistic MDPs and evluation criteria: finite-horizon MDPs evaluated on the expected reward of the created policies.

GOURMAND vastly outperformed IPPC-11 UCT-based probabilistic track winner PROST by far on goal-oriented domains Navigation and Crossing Traffic \cite{kolobov2012gourmand}. Prost however midly outperforms GOURMAND on the Game of Life, Sysadmin and the Traffic domain as UCT does not need to perform expensive Bellman Backups on these domains that include large branching factors.

\subsubsection{Navigation domain}
The navigation domain represents a grid in which a robot must reach a goal. Every cell offers the robot a different chance of dissapearinng. The robot needs to choose a path which gets it to the goal most reliably within the finite horizon time \cite{sanner2011navigation}. Because GOURMAND uses a dynamic lookahead, it performs quite well in this domain.

%ADD IMAGE HERE

\subsubsection{Traffic domain}
This domain represents a simple binary version of the cell transition model (CTM) for modelling traffic based on the original CTM Tech Report \cite{sanner2011traffic}. Because of the binary variables and no intermediate variable restrictions, this model is simplified and ignores traffic aspects such as turns and turn probabilities. This model also uses concurrent actions and the total actions has an upperbound of 2\^ (\# of intersections).

%ADD IMAGE HERE

%\subsection{IPPC-04 Domains}
%Duing IPPC-04, the domains were written in the Probabilistic PLanning Domain Description Language (PPDDL). 30 trials per problem were to be solved in 15 minutes.
%
%\subsubsection{}
%
%\subsubsection{}

\subsection{Real-World evaluation}

\subsubsection{Navigation domain}

In the navigation problem there is a chance that the robot disappears after
entering a cell. The disappearing action is a dead end. Disappearing here is a
metaphor for the robot breaking down, or that it disappears from the radar with
the base station. In many of such cases it's a big problem, but can be repaired
with a high cost. In the navigation domain a step to another cell has a cost of
1. If instead of a dead end the cost is $n$ (e.g. 5) instead of 1, which
represents the cost of repairing the robot, dead ends are removed. Without dead
ends the problem becomes probabilistic uninteresting, which makes it suitable for
replanning planners.

\subsection{Baseline test for Probabilistic Interesting domains}

%---------------------------------------------------------------------

\section{Future Experiment}

\subsection{Setup}

\subsection{FF-Replan}

\subsubsection{FF-Replan with Finite Horizon}

\subsection{Hypothesis}


\begin{itemize}
	\item Describe the experiment setup
	\item Show results for the experiment
	\item Critically analyze the results
	\item Compare with results from the original papers
\end{itemize}

%---------------------------------------------------------------------

\section{Related Work}

\subsection{RFF}

\subsection{PROST}

\subsection{FF-H+}

% Use related work to display the differences with own work

\begin{itemize}
	\item Discuss the related work and how their approaches are different
	\item Discuss the other domains and why our domain adds anything to the community
	\item Introduce the current state of the art papers that use deterministic approaches for probablistic domains dealing with uncertainty
\end{itemize}

\subsection{Probabilistic Planning via Determinization in Hindsight (2008)~\cite{yoon2008probabilistic}}

Hindsight optimization is an online technique that evaluates one-step-reachable
states by sampling future outcomes to generate multiple non-stationary planning
problems which are deterministic and can be used using search. It re-interprets
FF-Replan's~\cite{FFReplan} strategy randomly generating a set of non-stationary
determinized problems and combining their solutions.

\subsection{Improving Determinization in Hindsight for Online Probabilistic Planning (2010)~\cite{yoon2010improving}}
Hindsight optimization has enjoyed some success for online probabilistic
planning. Although it has proven to be effective for many domains, it is
computationally expensive. This paper presents significant improvements.  1) a
method for detecting potential useful actions, 2) exploit determinism in the
domain by reusing relevant plans, 2) improves action evaluation by increasing
the chance that at least one deterministic plan reaches a goal.

\subsection{Probabilistic Planning in the Graphplan Framework (1999)~\cite{Blum99probabilisticplanning}}
% might not be necessary?
Graphplan is a successful planning algorithm for classical STRIPS domains. This
paper explores the extend it can be used in \emph{probabilistic} domains. The
paper discusses two variations of Graphplan: PGraphplan and TGraphplan.
PGraphplan produces an optimal plan, while TGraphplan produces a sub-optimal
plan but it has increased speed. By comparing the speed and quality of the
two planners the authors are able to estimate how far they are from the ideal.

%\subsection{Compiling Conformant Probabilistic Planning Problems into Classical Planning (2013)~\cite{taig2013conformant}}
% not necessary?

\subsection{How Much Does a Household Robot Need To Know In Order To Tidy Up? (2013)~\cite{nebel2013much}}

For household robot planning, it appears easy, but actually involves
uncertainty. For tidying things up objects might not be there, or sensing
operations might tell things wrong. This paper looks into conditions for
classical planning in a replanning loop in order to solve nondeterministic
partially observable open domains This paper looks into conditions for
classical planning in a replanning loop in order to solve nondeterministic
partially observable open domains.

\subsection{Progressive heuristic search for probabilistic planning based on interaction estimates (2013)~\cite{martin2013progressive}}

In this paper a probabilistic plan graph heuristic is described which computes
information about the interaction between actions and between propositions.
This information is used to find better relaxed plans to compute the
probability of success. This information guides a forward state space search
for high probability, non-branching seed plans. These plans are then used in
a planning and scheduling system that handles unexpected outcomes by runtime
replanning.

%\subsection{Translation based approaches to probabilistic planning (2013)~\cite{taig2013translation}}
% I don't think one is valuable



\subsection{Probabilistic planning vs Replanning}
Inspired by the FF-Replan entrance in the IPPC competition, this paper described a probabilistic interestingness quality of domains. It then compares probabilistic planners with replanners in the different domains to draw conclusions. Classical planning with replanning upon failure when there is so little or so much uncertainty about the world that it is not worth modelling and reasoning about or when goals dynamically change or when the problem is too large to solve at once. Probabilistic planners are more appropriate when avoiding irreparable or costly failures. Probabilistic planning computes a plan that can handle all foreseeable contingencies and is preferable for quick reactions in closed control loops.

The following structural properties are identified for comparing probabilistic planning with replanning:
\begin{itemize}
	\item `dead' end states
	\item the degree to which the probability of reaching a dead end state can be reduced through choice of actions
	\item number of distinct trajectories
	\item presence of mutual exclusion
\end{itemize}

If there are no dead ends, a replanner will always succeed. It is impossible to improve on the unavoidable dead ends (positive probability of reaching when executing which cannot be reduced without reducing the probability of reaching the goal).

% repetition of what's already stated earlier

A probabilistic planning problem is considered to be probabilistically interesting if and only if it has all the following structural properties:

\begin{itemize}
	\item there are multiple goal trajectories;
	\item there is at least on pair of distinct goal trajectories, $\tau$ and $\tau$ ', that share a common sequence of outcomes for the first n-1 outcomes, and where $\tau$\textsubscript{n} and $\tau$'\textsubscript{n} are distinct outcomes of the same action; and
	\item there exist two distinct goal trajectories $\tau$ and $\tau$' and outcome o $\in$ $\tau$ and o' $\in$ $\tau$' of two distinct actions a = act(o) and a'= act(o') such that executing a strictly decreases the maximum probability of reaching a state where a' can be executed.
\end{itemize}

Problems that fail to be \emph{probabilistically interesting} are actually classical planning problems in disguise and for those replanners are more appropriate. A replanner can also perform optimally for probabilistically interesting planning problems whenever the `most promising' goal trajectory is the correct thing to do. They create several domains to explore the issue of replanning vs\@. probabilistic planning.

Following existing domains were said to be probabilistically interesting:
\begin{itemize}
	\item Drive
	\item Exploding blocksworld
	\item Pitchcatch
	\item Schedule
	\item Tireworld
\end{itemize}

They conclude by stating that the negative perception of probabilistic planning by the ICAPS community was inaccurate. They state that a synthesis of planning and replanning techniques could make a larger number of probabilistic planning problems practicably solvable than is currently possible.

%---------------------------------------------------------------------

\section{Conclusion}

\begin{itemize}
	\item Conclude on the success of the 2 approaches
	\item Conclude the work done in the paper
	\item Hint to some future work developments
\end{itemize}

%\section{Some papers}
%
%\begin{itemize}
%	\item \cite{Hoffmann01theff}
%	\item \cite{FFReplan}
%	\item \cite{teichteil2010incremental}
%	\item \cite{yoon2008probabilistic}
%	\item \cite{teichteil2012fast}
%	\item \cite{bonet2011planning}
%	\item \cite{taig2013translation}
%	\item \cite{martin2013progressive}
%	\item \cite{taig2013conformant}
%	\item \cite{2013workshop}
%	\item \cite{nebel2013much}
%	\item \cite{little2007probvsreplan}
%	\item \cite{keller2012prost}
%	\item \cite{kolobov2012glutton}
%	\item \cite{sanner2008rddl}
%	\item \cite{kolobov2012gourmand}
%\end{itemize}


\bibliographystyle{plain}
\bibliography{paper}

\end{document}
