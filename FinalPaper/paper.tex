\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{fixltx2e}
\usepackage{cite}

\usepackage{xcolor}
\newcommand\myworries[1]{\textcolor{red}{#1}}
\newcommand\todo[1]{\textcolor{blue}{#1}}

\usepackage{url}
\urldef{\mailsa}\path|{i.c.t.m.speek, a.c.stolwijk}@student.tudelft.nl|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip%
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter% start of an individual contribution

% first the title is needed
\title{Realistically Interesting Domains for Probabilistic Planners}

% a short form should be given in case it is too long for the running head
\titlerunning{Real World Domains}

\author{I.C.T.M Speek, A.C. Stolwijk}

%
\authorrunning{Presenting a }
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Seminar Algorithms, Embedded Software,
MSc Embedded Systems,\\
Delft University of Technology\\
\mailsa\\
}

%\toctitle{Lecture Notes in Computer Science}
%\tocauthor{Authors' Instructions}

\maketitle

%------------------------------------------------------------------------------


\begin{abstract}
By introducing and attempting to solve a planning problem domain involving Unmanned Aerial Vehicles (UAVs), this paper provides an overview of how the history of planning problem domains and their matching planners has changed throughout the scope of the International Planning Competitions (IPC). After the emergence of the FF-replan planner in 2004, state-of-the-art benchmark domains have been argued to be more \emph{probabilistically interesting} causing a shift from the once dominant goal-oriented domains to the finite-horizon oriented domains. 

This paper presents a currently hot topic domain in which several UAVs  work together to monitor a large area using restricted communication technology. To find an optimal planner for this domain, this paper presents an overview of other interesting domains and probabilistic planners that were involved in the redefinition of an interesting planning problem. 

\end{abstract}

%------------------------------------------------------------------------------

\section{Introduction}
The introduction of the International Planning Competition (IPC) in 1998 furthered the field of automated planning and scheduling. Since then several state-of-the-art planners such as the FF-planner \cite{Hoffmann01theff} have been introduced optimized for the initial classical planning problems. Surprisingly well-performing planners have caused the field to readjust the then supposedly challenging domains to provide a challenge for a wide variety of planners. By doing so, the definition of an interesting planning problem has been modified.

The entrance of FF-Replan \cite{FFReplan}, a replanner based on determinization techniques and the classical FF planner, surprisingly outperformed other participating MDP solvers in the International Probabilistic Planning Competition (IPPC) '04. As a result, Little et al. have argued for \emph{probabilistically interesting} domains \cite{little2007probvsreplan} and finite horizon MDPs were employed in IPPC-11. These approximations of infinite planning problems however strongly resemble optimization problems rather than the goal oriented initial planning domains.  This shift in the current interesting domains caused the state-of-the-art planners to cope with the large branching factors that are introduced using finite horizon MDPs.

This paper presents a new domain involving Unmanned Aerial Vehicles (UAVs) cooperating to deliver data packages from one UAV to the base station. To match this problem with an appropriate planner, this paper presents an overview of other interesting domains and probabilistic planners that were involved in the redefinition of an interesting planning problem. By evaluating the domains with the help of the probabilistic interestingness baseline test introduced by Little et al. \cite{little2007probvsreplan} a recommendation will be made for a suitable planner to match the UAV problem.

\todo{Arian: Still have to add why this problem is valid, or interesting}

Section \ref{sec:problem} introduced the problem formulation this paper addresses while Section \ref{sec:domaind} introduces the details of proposed domain. Section \ref{sec:background} introduced the appropriate prior knowledge and Section \ref{sec:domains} and \ref{sec:planners} the interesting domains and planners respectively. Section \ref{sec:experiment} briefly elaborates on a possible experiment set-up. Finally Section \ref{sec:conclusion} concludes on the evaluation of the domain and an appropriate planner.

%------------------------------------------------------------------------------

\section{Domain Description}
\label{sec:domaind}

Unmanned Aerial Vehicles (UAVs), or commonly called \emph{Drones} are aircrafts
without a pilot and are often autonomously controlled. Drones are widely used in the
military as well as for civil uses as inspection of pipelines, counting of
wildlife, agriculture, filming or delivering supplies to inaccessible areas.
There are many forms of UAVs to accommodate for these varying tasks: bigger fixed wing
aircrafts, small helicopters or quad copter like aircrafts that can hover at
a certain position in the air.

This paper focuses on monitoring, for example crops or livestock,
and surveillance UAVs that work together to monitor a large area. Each UAV
monitors a part of the total area. If something has to be monitored, it has
to communicate this to a base station. This can't be done directly due to the
large distance and instead it has to go through neighboring UAVs.

Communicating the message from the sensing UAV to the base station can be
expressed as a probabilistic planning problem. Initially the sensing UAV
has the package that has to be sent, and the goal is to bring this package to
the base station. Each time the package is passed to the neighboring UAV,
there is a possibility the package is not received correctly. The bigger the
distance, the higher the probability the delivery failed.

\myworries{maybe UAVs can be on or of too, so it's forced to use a further UAV.}

\myworries{what's also interesting is that Amazon.com uses drones for package deliveries}

%------------------------------------------------------------------------------

\section{Background}
\label{sec:background}

\subsubsection{Planning Problems. } Planning is the process of computing several steps of a problem-solving procedure before executing any of them. A \emph{classical planning problem} is defined as states, actions, conditions and a goal state and is solved using when finding a state-action sequence from the initial state $s_0$ to a goal state $G$. To find a state-action sequence, the environment is assumed to be \emph{deterministic, observable and static}. 

When planning for dynamic environments as is most often the case, natural dynamics and chance are introduced. To simulate these planning problems, the classical deterministic domains were extended by adding a \emph{probability} to every action, creating \emph{probabilistic planning problems}. These systems can be described using \emph{Markov Decision Processes} or MDPs. 

\subsubsection{Goal-oriented MDPs. }The classical goal-oriented problems in the IPPC-04 were specified using the Probabilistic Planning Domain Description Language (PPDLL). PPDDL specifies a probabilistic planning problem as a set of probabilistic actions and an initial and goal state description\cite{FFReplan}. These MDPs are of the form $\mathit{M = (S, A, app, Pr, R)}$ where $\mathit{S}$ and $\mathit{A}$ are a finite set of states and actions, $\mathit{app(s)}$ is the set of all actions applicable in the current state $\mathit{s}$. $\mathit{Pr}$ represents the probability of a state transition $\mathit{(s, a, s')}$ for action $\mathit{a}$ and $\mathit{R(s, a, s')}$ is the reward of applying $\mathit{a}$ to $\mathit{s}$ and generating $\mathit{s'}$ The planning problem is defined as a triple $\mathit{P = (M, s\textsubscript{0}, G}$ where $\mathit{M = (S, A, app, Pr, R)}$ is an MDP, $\mathit{s\textsubscript{0} \in S}$ is the initial state and $\mathit{G \subseteq S}$ is the set of goal states.

These planning problems were however relatively easy to solve as they did not account for large branching factors introduced by for instance decay \cite{kolobov2012glutton}. Natural dynamics cause the system to grow \myworries{exponentially??} as it introduces exogenous actions in every state. After FF-replan \cite{FFReplan}, a replanner as introduced in Section  \ref{sec:planners} kindly reminded the committee of IPPC-04 that the specified planning problems containing no dead-ends were fairly easy to solve using relatively simple tools to handle uncertainty, the finite-horizon MDPs were introduces. 

\subsubsection{MDPs with Finite Horizon. } Finite-horizon MDPs are an approximation for infinite planning problems using a finite horizon of a set amount of steps. Planning problems are defined as tuples of the form $\mathit{M(H) = \langle \langle S, A, T, R, s\textsubscript{0} \rangle , H \rangle }$ where $\mathit{S}$ is a finite set of states, $\mathit{A}$ a finite set of actions and $\mathit{T}$ is the transition function similar to $\mathit{Pr}$ in the goal oriented MDP. $\mathit{R}$ represents the transitioning reward, $\mathit{s\textsubscript{0}}$ the initial state and $\mathit{H}$ is the number of decision epochs after which the process stops. Solving $\mathit{M(H)}$ means finding a policy such that executing the recommended actions starting from state $(s_0, H)$ results in accumulating the largest expected reward over $H$ time steps \cite{kolobov2012glutton}.

\subsection{Planning algorithms}
 
These probabilistic MDPs can be solved using different kinds of planners. A \emph{replanner} is a planner that takes a problem as input, plans a partial state-action policy, executes it online and whenever it encounters a state that was not in its state-action policy, it takes the current state as starting state and plans again until it has reached a goal or dead-end state. To illustrate this technique, the notorious FF-planner is discussed in the next Section. 

\emph{Probabilistic planners} use sophisticated planning techniques to plan for uncertainty. By using simulations, roll-out policies or lookahead values, these planners prevent dead-ends and were therefore dominantly used for solving MDPs. To illustrate a probabilistic planner, the next section introduces \textsc{Gourmand}, an online state-of-the-art probabilistic planner, and its predecessors \textsc{Glutton} and the LRTDP based algorithms.

\subsection{Probabilistic Interestingness}
The \emph{probabilistically interesting} baseline test, proposed by Little and Thi\'ebauw \cite{little2007probvsreplan} after the success of FF-replan initiated the shift of interest in goal-oriented MDPs to finite-horizon MDPs. Problems that fail this test are said to be classical planning problems in disguise, favoring the use of replanners. A probabilistic planning problem is considered to be probabilistically interesting if and only if it has all the following structural properties \cite{little2007probvsreplan}:

\begin{itemize}
	\item there are multiple goal trajectories;
	\item there is at least one pair of distinct goal trajectories, $\tau$ and $\tau'$, that share a common sequence of outcomes for the first $n-1$ outcomes, and where $\tau_n$ and $\tau'_n$ are distinct outcomes of the same action; and
	\item At least one of the distinct goal trajectories lead to outcomes o $\in$ $\tau$ and o' $\in$ $\tau$' of two distinct actions $a = act(o)$ and $a'= act(o')$ such that executing $a$ decreases the probability of reaching a dead end without influencing the probability of reaching a goal state. \myworries{is dit duidelijk genoeg of is het niet wiskunding genoeg?}
\end{itemize}

%------------------------------------------------------------------------------

\section{Planners}
\label{sec:planners}

FF-Replan \cite{FFReplan}, a replanner based on determinization techniques and a classical planner, surprisingly outperformed other participating MDP solvers in the IPPC-04 and by doing so changed the course of the planning competition entirely. By relying on search and removing all probabilistic information from the problem, FF-replan functions with enormous speed and efficiency. This method has proven to outperform MDP solvers, the state-of-the-art probabilistic planners for uncertainty of that time. Since then, Little et al. has argued for \emph{probabilistically interesting} domains. By introducing finite horizon MDPs in IPPC-11, state of the art planners have been adapted to cope with large branching factors to approximate infinite planning problems. Because these ``planning problems'' can often be solved using a smaller horizon $h < H$ as introduced in Section \ref{sec:background}, they represent optimization problems rather than planning problems, favoring current state of the art planners over replanners. This section introduces the shift of the planners towards optimization  by discussing FF-replan and a current state of the art probabilistic planner \textsc{Gourmand} and its predecessors.

\subsection{FF-replan}
\label{sec:ffreplan}
 FF-Replan generates a deterministic planning problem by determinizing the input domain at the start of each planning iteration. This classical deterministic planning problem allows for the use of the classical FF planner \cite{Hoffmann01theff}. The FF planner then returns an ordered state-action plan for the deterministic problem of the current state. This plan is executed until an unexpected state is encountered in the following manner:

\begin{enumerate}
	\item Start planning at $s_0$
	\item Determinizes the input domain
	\item Use FF on the classical planning problem
	\item Executes ordered state-action plan until
	\item Unexpected state is encountered or
	\begin{itemize}
		\item Start planning at $s_t$ at time t
		\item Repeat step 2 - 6
	\end{itemize}
	\item Goal state or dead-end is reached
\end{enumerate}

FF-Replan has some obvious shortcomings. It removes all probabilistic information from the problem when determinizing the domain. The success of the planner is therefore fully reliable on the chosen determinization heuristic. As FF-replan uses no roll-out policy or future simulation, it has no way of avoiding \emph{avoidable} dead-ends. This explains why FF-replan performs poorly in domains containing dead-ends.

For a planner to be successful in finite-horizon MDPs it needs to be scalable and sensitive to an expected reward maximization, the evaluation criteria of these domains. To cope with these domains, trial based methods such as \emph{Labelled Real-Time Dynamic Programmers} (LRTDPs) have been introduced and optimized to find an optimal policy in a dynamic programming-based way.

%----------------------------------------------------------------------------------------

\subsection{LRTDP algorithms}

The LRTDP algorithm tries to reach the goal using a series of trials starting from $s_0$ and is specifically convenient for only memorizing states reachable from $s_0$ and therefore uses and needs less memory space in the following manner:

\begin{enumerate}
	\item Start trial at $s_0$
	\item Take action based on current value function $V$
	\item Perform Bellman backup
	\item Sample the outcome and update $V$
	\item Transition to the next state
	\item Repeat step 2 - 5 until a goal, dead end or converged state is reached
	\item Check if all states have converged, otherwise start a new trial (step 2 - 6)
\end{enumerate}

Adapting LRTDP to a finite horizon MDP $M(H)$ solver \emph{LRTDP-FH} requires for each trial to start at $s_0$ or $M(s_0, H)$ and run for at most $H$ time steps. All states $H$ steps away from $s_0$ would translate to goal states in a goal-oriented MDP. \\

The \emph{Labeled Reverse Real-Time Dynamic Programmer} (LR$^2$TDP) uses \emph{reverse iterative deepening} and LRTDP-FH to incrementally solve a sequence of MDPs $M(1)$, $M(2)$, \ldots , $M(H)$ by relying on the fact that a successful policy can be produced by solving a MDP-FH $M(h)$ for a smaller horizon $h < H$.

\begin{enumerate}
	\item Run LRTDP-FH to solve $M(1)$ or $M(s_0, 1)$
	\item Run LRTDP-FH to solve $M(h)$ or $M(s_0, h)$ where $h < H$ by backchaining from the horizon $H$
	\item Repeat step 2 until $H$ is reached or time has run out
\end{enumerate}
By iterating over the distance from the horizon the state values computed when solving $M(1)$, $M(2)$, \ldots, $M(h-1)$ can be reused. Solving $M(h)$ thus only takes an incremental effort over the solution of $M(h-1)$. If stopped prematurely  (LR$^2$TDP) is likely to provide a better policy as for many MDPs the optimal policy $\tau^*$ is optimal or near-optimal for $M(H)$. By itself,  (LR$^2$TDP) would however not perform well in MDPs with large branching factors as computing the Bellman backups would become too expensive carried out over a large fraction of the state space.

\subsection{\textsc{Glutton}}
The \textsc{Glutton} planner enhances LR$^2$TDP to cope with large branching factors by using several optimization techniques. It employs LR$^2$TDP in an off-line fashion by initializing the algorithm with the \emph{Max-Reward} heuristic that places an upper bound on the optimal value function $V^*$. It optimizes for large branching factors by performing the transition function in a subset of transition states $s'$. It distinguished between \emph{exogenous} effects caused by natural dynamics and \emph{pure} effects caused by the action itself and a mixed of both. It also utilized the saved memory space caused by mapping only the converged states in the hash-table by using the free space as cache for storing successor states computed using the transition function.
Since \textsc{Glutton} subsamples the transition function, it may not know a good action to perform in states it missed due to subsampling. To compensate for these states, \textsc{Glutton} has to resort to a \emph{default policy}. \todo{Wel of niet de default policy uitleggen}.

%------------------------------------------------------------------------------

When however dealing with large branching factors, offline planning is often a waste of time and memory space since there is a vast amount of states the planner may never reach in a lifetime. Such problems are far better solved using online planners that find an optimal policy $\tau^*$ or an approximation of this policy for the \emph{current} state.

\subsection{\textsc{Gourmand}}
The \textsc{Gourmand} is an online adaptation of the \textsc{Glutton} planner and ensures a good \emph{anytime performance} using a lookahead value $L$ to approximate a near-optimal policy for $M(H)$ where $L < H$ and is dynamically chosen. At every iteration over the distance to the horizon $H$, it divides the total remaining time $\overline{T}$ equally amongst the remaining iterations and virtually allocates time: $T_t = \frac{\overline{T}}{H - t}$. During the first epoch, or iteration it estimates how long solving a state takes for different lookahead values. Using this estimate and a virtual time allocation \textsc{Gourmand} determines the largest lookahead value $L_t$ for which it can almost certainly solve the current epoch  and whether it could solve for a larger lookahead $\widehat{L}_t$ by taking away time from the next epoch, still guaranteeing that it can solve both epochs. If so, it solves the current epoch for $\widehat{L}_t$ and otherwise $L_t$. This way \textsc{Gourmand} never has to resort to default policies and converges relatively fast to a good policy in an online fashion. The \textsc{Gourmand} planner has proven to outperform the current state of the art planners utilizing its robust any-time performance \cite{kolobov2012gourmand} in the following manner:

\begin{enumerate}
	\item Start \textsc{Gourmand} at $s_0$
	\item Initialization of first epoch
	\begin{enumerate}
		\item Virtually allocate time for  $T_t = \frac{\overline{T}}{H}$
		\item Run LR$^2$TDP($M_s(H), T_t$)
		\item Adjust running average $\overline{T}$
	\end{enumerate}
	\item Virtually allocate time  $T_t = \frac{\overline{T}}{H - t}$
	\item Check if lookahead can be increased $\widehat{L}_t = L_t + 1$ else $\widehat{L}_t = L_t $
	\item Run LR$^2$TDP($M_s(\widehat{L}), \widehat{T}_t$)
	\item Adjust running average $\overline{T}$
	\item Determine action based on (near) optimal $\tau^*$
	\item Transition to next state
	\item Repeat steps 3 - 8 until time has run out
\end{enumerate}

\todo{write some kind of conclusion}


%-------------------------------------------------------------------------------------------------------------------------------------

\section{Domains}
\label{sec:domains}

To identify unique properties of the domain and identify properties of the
domain that work particularly well for a certain type of planner, we describe
other domains used in the IPPC and compare that with the UAV domain.

\paragraph{Blocksworld:}
A simple domain used in the competition of 2004 is the well known blocksworld
domain. The uncertainty lies in that the gripper, used to move blocks, has a
probability that it drops the blocks to the table. Another difference is that
the blocks are colored, and the goal is to build a tower of certain colored
blocks, rather than a tower with specific blocks. If for example there are
four blue blocks, and the goal is a tower of three blue blocks, it does not
matter which blue blocks are used.

\paragraph{Navigation:}
The navigation domain represents a grid in which a robot must reach a goal.
Every cell offers the robot a different chance of disappearing. The robot needs
to choose a path which gets it to the goal most reliably within the finite
horizon time \cite{sanner2011navigation}.

\paragraph{Traffic:}
This domain represents a simple binary version of the cell transition model
(CTM) for modelling traffic based on the original CTM Tech Report
\cite{sanner2011traffic}. Because of the binary variables and no intermediate
variable restrictions, this model is simplified and ignores traffic aspects
such as turns and turn probabilities. This model also uses concurrent actions
and the total actions has an upper bound of $2^{\#~of~intersections}$.

\paragraph{Elevators:}
The elevators domain has evaluated between the competitions of 2006 and
2011, we describe the latest version. In this domain there are a number of
elevators delivering passengers to either the top or bottom floor. Potential
passengers arrive at any floor at a certain rate (depending on which floor).
The elevators can only move in the direction it is indicating, so passengers
know whether to board or not. The goal of the elevators is to transport as many
as possible passengers to their destinations.

\paragraph{SysAdmin:}
The SysAdmin domain describes a network of connected servers. Each server has
a probability that it shuts down, and if is not running, it has a probability
to reboot. The probability that a server shuts down depends on the number of
running connected servers.

\subsection{Domain Evaluation}

The Blocksworld domain is ideal for
replanning~\cite{FFReplan, little2007probvsreplan}. As the blocks fall on the
table, which then can be picked up again, it has no dead ends. This makes it
probabilistically uninteresting. For this domain FF-Repan performs really well.

The newer  domains, Navigation, Traffic, Elevators and SysAdmin are finite-
horizon domains. Navigation, however, is actually a goal oriented domain, if
the goal is reached in the horizon. The method how this is implemented is that
the goal state is defined as an absorbing state, and there is no reward or cost
associated to staying in the goal state, until the horizon is reached.

\textsc{Gourmand} vastly outperformed the UCT-based PROST, the IPPC-11
probabilistic track winner, by far on goal-oriented domains Navigation and
Crossing Traffic \cite{kolobov2012gourmand}. This is because of the dynamic
lookahead feature of \textsc{Gourmand}. PROST however mildly outperforms
\textsc{Gourmand} on the Game of Life, SysAdmin and the Traffic domain as UCT
does not need to perform expensive Bellman Backups on these domains that
include large branching factors.

The domains are constructed such that it will challenge planners to perform
better. This causes that there are issues with the domains when comparing them
with the real world.

In the navigation problem there is a chance that the robot
disappears after entering a cell. The disappearing action is a dead end.
Disappearing here is a metaphor for the robot breaking down, or that it
disappears from the radar with the base station. In many of such cases it's a
big problem, but can be repaired with a high cost. In the navigation domain a
step to another cell has a cost of 1. If instead of a dead end the cost is $n$
(e.g. 5) instead of 1, which represents the cost of repairing the robot, dead
ends are removed. Without dead ends the problem becomes probabilistic
uninteresting, which makes it suitable for replanning planners.

The traffic domain describes a crossroad with traffic lights, modelled as
cells. Either the north to south road has a green light, the west to east road
or all lights are red. The model uses a finite state machine that handles the
lights so that the lights are never green at the same time. This would be the
only dangerous situation on the crossroad. The difficulty in this domain is
the number of states, which grows exponentially with the number of cells.
Besides that it is an uninteresting probabilistic problem because it does
not satisfy the probabilistic interesting test.

The UAV domain has a resemblance of the Navigation domain. Both are goal
oriented and have to navigate something (either the robot itself, or a data
package) from an initial state to a goal. Also after each step a dead end can
be reached, the robot disappears or the data package is not delivered.
The probability this happens is static in the navigation domain, but dependent
on the neighbors in the UAV domain. This variable probability has a resemblance
of the SysAdmin domain, where the probability that a server shuts down is also
dependent on the neighbors.


%------------------------------------------------------------------------------

\section{Related Work}

%In sections above we have described the GOURMAND planner and FF-Replan as two
%probabilistic planners of the IPPC competition.
%RFF~\cite{teichteil2010incremental} is an offline planner that generates a
%policy: for each reachable state it determines a plan to the goal. Like
%FF-Replan it uses determinization and FF\. FF returns an initial plan from the
%initial state to a goal state. For each state in the plan the successor states
%are added to the structure, after which Monte-Carlo sampling is used. States
%where the probability that they're reached is too low (below a threshold) are
%removed. The successor states are used as initial states in the next step. If
%FF fails to find a plan to the goal from this new state, the state is removed
%from the structure too. Results showed that RFF outperformed other competitors.
%A higher threshold means that RFF will generate more policies that are prone to
%replanning.

\subsubsection{PROST ?}
\todo{wrote sentence about Prost - Arian}\\

\todo{rewrite prob int - Imara}

%\subsection{Probabilistic planning vs Replanning}
%Inspired by the FF-Replan entrance in the IPPC competition, this paper described a probabilistic interestingness quality of domains. It then compares probabilistic planners with replanners in the different domains to draw conclusions. Classical planning with replanning upon failure when there is so little or so much uncertainty about the world that it is not worth modelling and reasoning about or when goals dynamically change or when the problem is too large to solve at once. Probabilistic planners are more appropriate when avoiding irreparable or costly failures. Probabilistic planning computes a plan that can handle all foreseeable contingencies and is preferable for quick reactions in closed control loops.
%
%The following structural properties are identified for comparing probabilistic planning with replanning:
%\begin{itemize}
%	\item `dead' end states
%	\item the degree to which the probability of reaching a dead end state can be reduced through choice of actions
%	\item number of distinct trajectories
%	\item presence of mutual exclusion
%\end{itemize}
%
%If there are no dead ends, a replanner will always succeed. It is impossible to improve on the unavoidable dead ends (positive probability of reaching when executing which cannot be reduced without reducing the probability of reaching the goal).
%
%% repetition of what's already stated earlier
%
%A probabilistic planning problem is considered to be probabilistically interesting if and only if it has all the following structural properties:
%
%\begin{itemize}
%	\item there are multiple goal trajectories;
%	\item there is at least on pair of distinct goal trajectories, $\tau$ and $\tau$ ', that share a common sequence of outcomes for the first n-1 outcomes, and where $\tau$\textsubscript{n} and $\tau$'\textsubscript{n} are distinct outcomes of the same action; and
%	\item there exist two distinct goal trajectories $\tau$ and $\tau$' and outcome o $\in$ $\tau$ and o' $\in$ $\tau$' of two distinct actions a = act(o) and a'= act(o') such that executing a strictly decreases the maximum probability of reaching a state where a' can be executed.
%\end{itemize}
%
%Problems that fail to be \emph{probabilistically interesting} are actually classical planning problems in disguise and for those replanners are more appropriate. A replanner can also perform optimally for probabilistically interesting planning problems whenever the `most promising' goal trajectory is the correct thing to do. They create several domains to explore the issue of replanning vs\@. probabilistic planning.
%
%Following existing domains were said to be probabilistically interesting:
%\begin{itemize}
%	\item Drive
%	\item Exploding blocksworld
%	\item Pitchcatch
%	\item Schedule
%	\item Tireworld
%\end{itemize}
%
%They conclude by stating that the negative perception of probabilistic planning by the ICAPS community was inaccurate. They state that a synthesis of planning and replanning techniques could make a larger number of probabilistic planning problems practicably solvable than is currently possible.



%------------------------------------------------------------------------------

\section{Conclusion}

\begin{itemize}
	\item Conclude the work done in the paper
	\item Hint to some future work developments
\end{itemize}

%------------------------------------------------------------------------------

\section{Future works}



\bibliographystyle{plain}
\bibliography{paper}

\end{document}
