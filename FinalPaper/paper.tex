\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{fixltx2e}

\usepackage{url}
\urldef{\mailsa}\path|{i.c.t.m.speek, a.c.stolwijk}@student.tudelft.nl|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip%
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter% start of an individual contribution

% first the title is needed
\title{Validating FF-Replan vs.\@ a State-Of-The-Art Probabilistic Planner in Translated Real World Domains}

% a short form should be given in case it is too long for the running head
\titlerunning{FF-Replan vs Probabilistic Planner in Real World Domains}

\author{I.C.T.M Speek, A.C. Stolwijk}

%
\authorrunning{Validating Planners in a Real World Domain}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Seminar Algorithms, Embedded Software,
MSc Embedded Systems,\\
Delft University of Technology\\
\mailsa\\
}

%\toctitle{Lecture Notes in Computer Science}
%\tocauthor{Authors' Instructions}

\maketitle

%--------------------------------------------------------------------

\begin{abstract}
	In contrast to current demand for probabilistically interesting domains in the International Probabilistic Planning Competitions, this papier argues for using real-world appropriate domains. By relaxing the definition of a dead-end, we argue that for many
real life domains a simpler planning algorithm can be sufficient and even better than sophisticated probabilistic planners.

In this paper we evaluate the strength of FF-Replan, winner of IPPC-04 and unenrolled winner of IPPC-06, a replanner that uses the classical planner FF on a determinized probabistic problem against a state of the art probabilistic planner GOURMAND, the successor of the Labelled Real-Time Dynamic Probabilistic planner Glutton, runner up of the IPPC-11. By translating the Navigation and Traffic domain from IPPC-11 to a matching real-world equavalent and evaluating these using the proposed probabilistically interesting baseline test as proposed by Little et. all we will evaluate whether FF-Replan is in fact capable enough to solve enough of these so called classical planning problems in disguise.
\end{abstract}

%--------------------------------------------------------------------

\section{Introduction}

Since the first the International Probabilistic Planning Competition (IPPC),
problems have undergone radical changes. Dominating planners of previous
incorporations had a strong focus on goal-oriented MDPs, determinizing and
using powerful classical planners (e.g. FF \cite{Hoffmann01theff}) and
replanning. New benchmark problems have demonstrated weaknesses in existing
solution techniques.

Little and Thi\'ebaux have developed a baseline test in
\cite{little2007probvsreplan} to evaluate the problems of the fifth International
Planning Competition (IPC-5). This test determines if a planning problem is
`probabilistically interesting'. Problems that fail this test are classical
planning problems in disguise. Knowing this it is easy to design a problem
where a replanner performs badly or incredibly well.

The IPPC-11 introduced Relation Dynamic Influence Diagram Language (RDDL),
which can design domains that are more probabilistically interesting. The top
performers were PROST \cite{keller2012prost} and Glutton
\cite{kolobov2012glutton}.

While it is interesting to make problems harder and invent better planning
algorithms to solve those probabilistic complex problems, we evaluate whether
or not these problems are coherent with real-world situations.

The main contribution of this paper is an evaluation of FF-Replan versus a
probabilistically state of the art planner GOURMAND in the realistically
augmented domains of IPPC-11. If these domains fail the baseline test proposed
by Little et\@. all, they would translate to classical planning problems in
disguise and could be easily solved by FF-Replan.

%\begin{itemize}
%	\item Describe probabilistic planning problem
%	\item Introduce planning for uncertainty
%	\item Describe how deterministic classical planners can be used in probabilistic domains and motivate why this is interesting
%	\item Introduce the qualities of replanning and the second planning algorithm
%	\item Describe the need for a new domain for real world applications
%	\item Describe the goal and tools used to reach this goal
%\end{itemize}

%---------------------------------------------------------------------

\section{Problem formulation}
To evaluate FF-Replan and GOURMAND in a real world domain a finite horizon MDP is used to represent the realistic \emph{natural dynamics} such as external forces or forces of nature. These dynamics introduce large branching factors emphasizing the need for a planner to be scalable. To evaluate the model, the planner should also be sensitive to an expected reward maximization criterion.
Current finite horizon MDP problems as introduced in IPPC-11 are simplified and therefor do not account for the rich set of state-actions as would be available in a real-world planning problem. We will reason about the most interesting domains for our evaluation and adjust them to match their real-world equivalent. Using the baseline test introduced by Little et. all \cite{little2007probvsreplan}, we can evaluate the probabilistically interestingness of these domains and evaluate the performance of FF-Replan vs. a state of the art probabilistic planner: GOURMAND.



%---------------------------------------------------------------------

\section{Background}

\subsection{Planning problems}
\subsubsection{Classical Planning problems. } Planning is the process of computing several steps of a problem-solving procedure before executing any of them. In classical planning problems the environment is assumed to \emph{deterministic, observable and state}. Classical planning problems consists out of states, actions, conditions and a goal state and are solved using search and can be aided using heuristics. The planner tries to find a sequence of actions such that a goal or maximum reward is achieved.

\subsubsection{Probabilistic Planning problems}
Real-World applications are far from deterministic, induced by natural dynamics and chance. To accommodate for Real-World domains probabilistic planning problems were introduced.  These systems can be described using \emph{Markov Decision Processes} (MDPs) and can account for observable uncertainty. Probabilistic planning problems can be solved by evaluating iterations and creating policies to reach a goal state, either online or offline.

\subsection{Problem domains}
\subsubsection{Goal-oriented MDPs. }The problems in IPPC-04 were specified using the Planning Domain Description Language (PPDLL). PPDDL specifies a probabistic planning problem as a set of probabilistic action sets and an initial and goal state description in which every action has a certain probability assigned to it \cite{FFReplan}. These MDPs are of the form $\mathit{M = (S, A, app, Pr, R)}$ where $\mathit{S}$ and $\mathit{A}$ are a finite set of states and actions, $\mathit{app(s)}$ is the set of all actions applicable in $\mathit{s}$. $\mathit{Pr}$ represents the probability of a state transaction $\mathit{(s, a, s')}$ for $\mathit{a}$ and $\mathit{R(s, a, s')}$ is the reward of applying $\mathit{a}$ to $\mathit{s}$ and generating $\mathit{s'}$ The planning problem is defined as a triple $\mathit{P = (M, s\textsubscript{0}, G, \rho)}$ where $\mathit{M = (S, A, app, Pr, R)}$ is an MDP, $\mathit{s\textsubscript{0} \in S}$ is the initial state, $\mathit{G \subseteq S}$ is the set of goal states and $\mathit{0 < \rho < 1}$ is a probability threshold \cite{teichteil2010incremental}.

 The planners from IPPC-04 were evaluated on how many trials reached the goal per problem and the average number of actions selected when successful. These planning problems were however relatively easy to solve as they did not account for \emph{natural dynamics} and/or large branching factors that occur in more realistic systems \cite{kolobov2012glutton}. FF-Replan \cite{FFReplan}, as specified in Section \ref{sec:planners} kindly remembered the committee of the IPPC-04 that the specified planning problems that contained no dead-ends were fairly easy to solve using relatively simple tools to handle with uncertainty.

\subsubsection{MDPs with Finite Horizon. }IPPC-11 decided to respond by introducing more realistic MDPs and their evaluation criteria by using MDPs with finite horizons. The planning problems are defined as tuples of the form \\ $\mathit{M\textsubscript{s0}(H) = \langle \langle S, A, T, R, s\textsubscript{0} \rangle , H \rangle }$ where $\mathit{S}$ s a finite set of states, $\mathit{A}$ a finite set of actions, $\mathit{T}$ is the transition function, similar to $\mathit{Pr}$ in the goal oriented MDP. $\mathit{R}$ represents the transitioning reward, $\mathit{s\textsubscript{0}}$ and $\mathit{H}$ is the number of decision epochs after which the process stops. Solving $\mathit{M(H)}$ means finding a policy for executing recommended state from the initial state over $\mathit{H}$ decision epochs \cite{kolobov2012gourmand}.

Translating finite horizon MDPs to goal-oriented domains results ends up in all state $H$ steps away from $s\textsubscript{0}$ are goal states. The evaluation criteria in finite horizon MDPs is the \emph{expected reward} and not the probability of the policy reaching the goal states.

\subsection{Planning algorithms}
\subsubsection{Replanning}
A replanner is a planner that takes a problem as input, plans a state-action policy, executes it online and whenever it encounters a state that was not in its state-action policy, it takes the current state as starting state and plans again until it has reached a goal or dead-end state. For our paper have chosen to evaluate FF-Replan \cite{FFReplan}, the winner of IPPC-04 and unenrolled winner of IPPC-06 as an inspiring, fairly simple planner that in our opinion is capable of employment in real-world domains.

\subsubsection{Probabilistic planning}
Probabilistic planners are used to solve probabilistic planning problems and should be able to account for uncertainty as opposed to classical planners. For our evaluation we have chosen to compare FF-Replan to a state-of-the-art probabilistic planner: GOURMAND. GOURMAND is the online successor of the GLUTTON plan, a Labelled Real-Time Dynamic Programming (LRTDP) based planner and the runner up of IPPC-11.

\subsection{Probabilistic Interestingness}
After IPPC-06, a baseline test for \emph{probabilistically interesting} planning domains was proposed by Little and Thi\'ebauw \cite{little2007probvsreplan}.  Problems that fail this test were deemed to be classical planning problems in disguise, favouring the use of replanners. A probabilistic planning problem is considered to be probabilistically interesting if and only if it has all the following structural properties \cite{little2007probvsreplan}:
% reference little & thiebauw again. Maybe also shortly define what all o, n, act etc. are.

\begin{itemize}
	\item there are multiple goal trajectories;
	\item there is at least on pair of distinct goal trajectories, $\tau$ and $\tau$ ', that share a common sequence of outcomes for the first n-1 outcomes, and where $\tau$\textsubscript{n} and $\tau$'\textsubscript{n} are distinct outcomes of the same action; and
	\item there exist two distinct goal trajectories $\tau$ and $\tau$' and outcome o $\in$ $\tau$ and o' $\in$ $\tau$' of two distinct actions a = act(o) and a'= act(o') such that executing a strictly decreases the maximum probability of reaching a state where a' can be executed.
\end{itemize}

%Following existing domains were said to be probabilistically interesting:
%
%\begin{itemize}
%	\item Drive
%	\item Exploding blocksworld
%	\item Pitchcatch
%	\item Schedule
%	\item Tireworld
%\end{itemize}


%---------------------------------------------------------------------

\section{Planners}
\label{sec:planners}

\subsection{FF-Replan}

At the first international probabilistic planning competition IPPC-04, most entries were Markov Decision Processes. The winner, FF-Replan, however was based on deterministic planning techniques.

FF-Replan is an action selection algorithm for online planning in probabilistic domains. Given a domain description, a goal and an initial state as a probabilistic planning problem, FF-Replan generates a deterministic planning problem. It determinizes the input domain at the start of the planning into a deterministic different domain. They consider two methods of determinization:
\begin{enumerate}
\item \emph{Single-outcome determinization}: selects one outcome for each probabilistic construct using a variety of heuristics. The performance strongly depends on the methods for choosing the outcomes.
\item \emph{All-outcomes determinization}: considers every probabilistic outcome as a distinct deterministic action. For each possible outcome FF-Replan generates one seperate action for each effect. For nested probabilities, the all-outcomes procedure is applied recursively. This implies that FF-Replan will cease selecting actions whenever it enters a dead state. However when given no information, all probabilistic effects will be considered as equal which is a potential weakness.
\end{enumerate}

After determinization FF-Replan then uses the deterministic FF planner to compute an ordered plan for the generated deterministic problem. This process is repeated when countering an unexpected state until a goal state is reached. It maintains a partial state-action mapping using a hash-table which is initially empty. Whenever FF-Replan encounters a state which is not in the table, it determinizes the problem and synthesizes a plan using FF. It then simulates the plan according to the deterministic action definitions resulting in a state-action sequence whose pairs are put in the hash table. The first action of the plan is then executed in the environment, returning a new state. This partial policy is this produces in an online fashion. It would be valuable to look into incremental planning approaches that don't create a deterministic plan from scratch whenever an unexpected state is encountered or reusing previous planning effort.

FF-Replan has some obvious shortcomings as it first determinizes the input domain, removing all probabilistic information from the problem and then synthesizes a plan. If an unexpected state should occur whilst executing, the planner replans in the same determinization. It also does not consider multiple potential effects if an action.

By relying on search, FF-Replan could function with enormous speed and efficiency. However when dealing with difficult probabilistic problems attempting to span the spectrum of complexity ranging from deterministic problems to highly stochastic problems, FF-Replan is expected to come short. Because FF-Replan is developed hoping to inspire extensions and insight into the approach and planning domains, the authors are hoping to achieve this.


\subsection{GOURMAND}

The IPPC-2011 introduces more realistic MDPs to accommodate real-world systems with large branching factors cause by \emph{natural dynamics} and a limited time for the planner to come up with a policy. For this reason planners needed to be scalable and sensitive to the expected reward maximization. Glutton introduces an off-line algorithm based on LR\textsuperscript{2}TDP and relies on the fact that a successful policy can be produced by solving an MDP $M(H)$ for a smaller horizon $h$. Incrementally solving the sequence $M(1)$, $M(2)$ forms the basis of LR\textsuperscript{2}TDP\@. By iterating on the distance from the horizon, data can be reused (and is stored in the cache until the hash-table is full). The max-reward heuristic finds the largest horizon value $h'$ for which already a heuristic value $V$ is estimated. The  computation time remains much lower because it runs quickly into states that converged while solving $M(h-1)$. The max-reward computation relies on knowing the maximum $R_{max}$ any action can yield in any state or an upper bound on it.

\subsubsection{LRTDP}
Using goal oriented MDPs, the \emph{Labelled Real-Time Dynamic Programmer} (LRTDP) operates a series of trials starting from $s_0$.

\begin{itemize}
	\item Trials start at $s_0$
	\item Choose best action greedily in the current state based on current $V$
	\item Perform  Bellman backup on the current state
	\item Sample the outcome as the chosen action
	\item Transition to the new state
	\item Repeat until a goal, dead end or converged state is reached
	\item After trial, special convergence check on all states in the trial is performed to prove the convergence of these values
\end{itemize}

\subsubsection{LRTDP-FH}

Adapting LRTDP to a finite horizon MDP \emph{M(H)} requires to let each trial start at ($s_0$) and run for at most H time steps. A steps $H$ steps away from $s_0$ would translate to goals in a goal-oriented MDP. However, it's anytime performance to meet the demands of a dynamic environment is not very good, creating the need for a more sophisticated approach \cite{kolobov2012glutton}.

\subsubsection{LR\textsuperscript{2}TDP}

LR\textsuperscript{2}TDP uses \emph{reverse iterative deepening} and LRTDP-FH to solve a sequence of MDPs $M(1)$, $M(2)$, \ldots , $M(H)$, in that order. It first decides how to act optimally in ($s_0$, 1), equivalent to solving M(1) , then ($s_0$, 2), equivalent to solving $M(2)$, eventually solving $M(H)$ or running out of time halting a $M(h')$ where $h' < H$. This incremental effort allows for reusing state values computed in previous states, so it memorizes all values and solves for smaller horizons by backchaining from the goal LRTDP-FH iterates over search depth while LR\textsuperscript{2}TDP iterates over the distance from the horizon and is likely to be better because:

\begin{itemize}
	\item In many MDPs, the optimal policy $M(h)$ for some $h < H$ is near optimal for M(H), it thus may have a solution when stopped prematurely
	\item it's trials are very short because backchaining from the goal leads to a lots of convergence
	\item It optimally memorizes a large fraction of augmented states to save memory
	\item a large fraction of state space may be unreachable from $s_0$ and it avoid storing information it does not need
	\item it should however use a heuristic to treat states that has never been encountered when stopped prematurely, could be a fall back or a default action.
\end{itemize}

\subsubsection{GLUTTON}

Glutton uses the \emph{Max-Reward} heuristic that relies on knowing the maximum reward $R_{max}$ any action can yield in a state or an upper bound on it. Large branching factors induce that the computation of the Bellman Backups becomes prohibitively expensive. Glutton uses the following mechanisms to optimize for large branching factors:

\begin{itemize}
	\item Subsampling the transition function, is not enough to decrease the amount of computations yet
	\item Separating out Natural Dynamics: to reuse variable samples when generator successors for multiple actions, by separating the actions' effects into natural dynamics (\emph{exogenous}), those due to the action itself (\emph{pure effects}) and those do a mix of both.
	\item Caching the transition function samples: as LR\textsuperscript{2}TDP populates the state-value hash table lazily, Glutton uses  this memory as a cache for samples from the transition function, so it doesn't have to regenerate the action when encountering the same $(s,h)$ pair. When full, it chooses state-action pars for eviction ans replacement randomly.
	\item Default policies: Glutton uses 3 tricks to determine a policy when stopped prematurely. It compares the optimal action from a $h'$ with a simple cyclic policy or a complete random policy and chooses the best of these based on the reward.
\end{itemize}
%---------------------------------------------------------------------
During IPPC-2011 1 out of 5 planners used a LRTDP-based system and managed to outperform three other planners using the dominant algorithm for probabilistic planning: UCT. This caused to wonder whether LRTDP planners could outperform UCT based systems.  UCT forces an expert to guess a non-shortsighted lookahead value for which it should be able to converge on encountered states. Choosing this value can have very bad consequences and its existence causes the UCT to require some input, taking away from its autonomicity. At the IPPC-2011 GLUTTON was an offline LRTDP based planner that came second. As PROST employed UCT in an online manner, GOURMAND is an online adaptation of the GLUTTON algorithm to test whether LRTDP performs better if it is employed online.

UCT picks an action based on it current quality estimate and an \emph{exploration term} which forces the planner to choose actions that have rarely been tried in the past. This makes UCT suitable for a wider scale of scenarios but also implies an unreliable termination condition.

Because real world domains contain Natural Dynamics that introduce large branching factors, offline planning is often a waste of time and memory space since there is a vast amount of state the planner may never reach in a lifetime. Such problems are far better solved using online planners by finding a $\tau$* or an approximation for the current state, executing it and repeating. This is done by choosing a certain lookahead  L and solving MDP \emph{M\textsubscript{s}(L)} to solve \emph{M\textsubscript{0}(H)} where L $<$ H and then select \emph{$\pi$*\textsubscript{L}(s, L)} recommended by the optimal policy. This lookahead value is determined using the per-process time constraint.

\subsubsection{GOURMAND}
In contrast to GLUTTON, GOURMAND always makes an informed choice in any state where it ends up when the time T allocated for solving the MDP runs out because of the online fashion it uses LRTDP. GOURMAND first divides the time among all remaining epochs, determines the largest lookahead L for which it can almost certainly solve any remaining epoch if T is assigned.  It then checks if it can solve even for a larger lookahead L by taking away a small configuration time from future epochs  while still guaranteeing that it can solve for Lookahead L in each of them. This was GOURMAND adaptively picks a good lookahead value at each desicion epoch given the per-process constraint T. This can only be achieved by knowing how long it takes for LRTDP to solve a state for certain lookahead values. It uses LR\textsuperscript{2}TDP to collect this data. This version of LRTDP is more efficient because of the early convergences and it allows measuring how long on average the solving of a task takes. GOURMAND uses the same engineering optimizations to to handle evaluate actions, iterating over the successors of s under a by the Bellman Backups. GOURMAND eventually outperforms both GLUTTON and PROST, the winner of the IPPC-11\cite{keller2012prost}.


%---------------------------------------------------------------------

\section{Domains}
Following the theoretical comparison of probabilistic planning and replanning techniques by Little and Thi\'ebauw, a baseline test for \emph{probabilistically interesting} domains is proposed. Problems that fail this test were deemed to be classical planning problems in disguise, favouring the use of replanners \cite{little2007probvsreplan}. This section describes interesting domains in which either FF-Replan or GOURMAND prevailed during the International Probabilistic Planning Competitions. These domains are evaluated for how well they portray real-world scenarios. The domains we deem most interesting for the purpose of this paper are adjusted to match their real-world scenario. Little et all's baseline test helps us to predict which planner would perform well on these domains. We will then conclude our evaluation by comparing these results to the result of our experiments described in Section \ref{sec:experiment}.

\subsection{IPPC-2011 Domains}

The competitor FF-Replan \cite{FFReplan} kindly remembered the committee of the IPPC-04 that problems containing no dead ends and action costs are fairly easy to solve without much handling of uncertainty. The baseline test of Little et. all evaluated the competition problems of the IPPC described in PPDDL to correct the possible inaccurate perception of replanners vs.\@ probabilistic planners. IPPC-2011 responded by introducing more realistic MDPs and evaluation criteria: finite-horizon MDPs evaluated on the expected reward of the created policies.

GOURMAND vastly outperformed IPPC-11 UCT-based probabilistic track winner PROST by far on goal-oriented domains Navigation and Crossing Traffic \cite{kolobov2012gourmand}. PROST however mildly outperforms GOURMAND on the Game of Life, Sysadmin and the Traffic domain as UCT does not need to perform expensive Bellman Backups on these domains that include large branching factors.

\subsubsection{Navigation domain}

The navigation domain represents a grid in which a robot must reach a goal. Every cell offers the robot a different chance of disappearing. The robot needs to choose a path which gets it to the goal most reliably within the finite horizon time \cite{sanner2011navigation}. Because GOURMAND uses a dynamic lookahead, it performs quite well in this domain.

%ADD IMAGE HERE

\subsubsection{Traffic domain}
This domain represents a simple binary version of the cell transition model (CTM) for modelling traffic based on the original CTM Tech Report \cite{sanner2011traffic}. Because of the binary variables and no intermediate variable restrictions, this model is simplified and ignores traffic aspects such as turns and turn probabilities. This model also uses concurrent actions and the total actions has an upperbound of 2\textsuperscript{ (\# of intersections)}.

%ADD IMAGE HERE

%\subsection{IPPC-04 Domains}
%Duing IPPC-04, the domains were written in the Probabilistic PLanning Domain Description Language (PPDDL). 30 trials per problem were to be solved in 15 minutes.
%
%\subsubsection{}
%
%\subsubsection{}

\subsection{Real-World evaluation}

\subsubsection{Navigation domain}

In the navigation problem there is a chance that the robot disappears after
entering a cell. The disappearing action is a dead end. Disappearing here is a
metaphor for the robot breaking down, or that it disappears from the radar with
the base station. In many of such cases it's a big problem, but can be repaired
with a high cost. In the navigation domain a step to another cell has a cost of
1. If instead of a dead end the cost is $n$ (e.g. 5) instead of 1, which
represents the cost of repairing the robot, dead ends are removed. Without dead
ends the problem becomes probabilistic uninteresting, which makes it suitable for
replanning planners.

\subsection{Baseline test for Probabilistic Interesting domains}

%---------------------------------------------------------------------

\section{Future Experiment}
\label{sec:experiment}

\subsection{Setup}

\subsection{FF-Replan}

\subsubsection{FF-Replan with Finite Horizon}

\subsection{Hypothesis}

%---------------------------------------------------------------------

\section{Related Work}

\subsubsection{RFF}. Also uses an incremental plan aggregation, but offline similar to GLUTTON.

\subsubsection{PROST}

\subsubsection{FF-H+}

%\subsection{Probabilistic Planning via Determinization in Hindsight (2008)~\cite{yoon2008probabilistic}}
%
%Hindsight optimization is an online technique that evaluates one-step-reachable
%states by sampling future outcomes to generate multiple non-stationary planning
%problems which are deterministic and can be used using search. It re-interprets
%FF-Replan's~\cite{FFReplan} strategy randomly generating a set of non-stationary
%determinized problems and combining their solutions.

%\subsection{Improving Determinization in Hindsight for Online Probabilistic Planning (2010)~\cite{yoon2010improving}}
%Hindsight optimization has enjoyed some success for online probabilistic
%planning. Although it has proven to be effective for many domains, it is
%computationally expensive. This paper presents significant improvements.  1) a
%method for detecting potential useful actions, 2) exploit determinism in the
%domain by reusing relevant plans, 2) improves action evaluation by increasing
%the chance that at least one deterministic plan reaches a goal.

%\subsection{Probabilistic Planning in the Graphplan Framework (1999)~\cite{Blum99probabilisticplanning}}
%% might not be necessary?
%Graphplan is a successful planning algorithm for classical STRIPS domains. This
%paper explores the extend it can be used in \emph{probabilistic} domains. The
%paper discusses two variations of Graphplan: PGraphplan and TGraphplan.
%PGraphplan produces an optimal plan, while TGraphplan produces a sub-optimal
%plan but it has increased speed. By comparing the speed and quality of the
%two planners the authors are able to estimate how far they are from the ideal.

%\subsection{Compiling Conformant Probabilistic Planning Problems into Classical Planning (2013)~\cite{taig2013conformant}}
% not necessary?

%\subsection{How Much Does a Household Robot Need To Know In Order To Tidy Up? (2013)~\cite{nebel2013much}}
%
%For household robot planning, it appears easy, but actually involves
%uncertainty. For tidying things up objects might not be there, or sensing
%operations might tell things wrong. This paper looks into conditions for
%classical planning in a replanning loop in order to solve nondeterministic
%partially observable open domains This paper looks into conditions for
%classical planning in a replanning loop in order to solve nondeterministic
%partially observable open domains.

%\subsection{Progressive heuristic search for probabilistic planning based on interaction estimates (2013)~\cite{martin2013progressive}}
%
%In this paper a probabilistic plan graph heuristic is described which computes
%information about the interaction between actions and between propositions.
%This information is used to find better relaxed plans to compute the
%probability of success. This information guides a forward state space search
%for high probability, non-branching seed plans. These plans are then used in
%a planning and scheduling system that handles unexpected outcomes by runtime
%replanning.

%\subsection{Translation based approaches to probabilistic planning (2013)~\cite{taig2013translation}}
% I don't think one is valuable



\subsection{Probabilistic planning vs Replanning}
Inspired by the FF-Replan entrance in the IPPC competition, this paper described a probabilistic interestingness quality of domains. It then compares probabilistic planners with replanners in the different domains to draw conclusions. Classical planning with replanning upon failure when there is so little or so much uncertainty about the world that it is not worth modelling and reasoning about or when goals dynamically change or when the problem is too large to solve at once. Probabilistic planners are more appropriate when avoiding irreparable or costly failures. Probabilistic planning computes a plan that can handle all foreseeable contingencies and is preferable for quick reactions in closed control loops.

The following structural properties are identified for comparing probabilistic planning with replanning:
\begin{itemize}
	\item `dead' end states
	\item the degree to which the probability of reaching a dead end state can be reduced through choice of actions
	\item number of distinct trajectories
	\item presence of mutual exclusion
\end{itemize}

If there are no dead ends, a replanner will always succeed. It is impossible to improve on the unavoidable dead ends (positive probability of reaching when executing which cannot be reduced without reducing the probability of reaching the goal).

% repetition of what's already stated earlier

A probabilistic planning problem is considered to be probabilistically interesting if and only if it has all the following structural properties:

\begin{itemize}
	\item there are multiple goal trajectories;
	\item there is at least on pair of distinct goal trajectories, $\tau$ and $\tau$ ', that share a common sequence of outcomes for the first n-1 outcomes, and where $\tau$\textsubscript{n} and $\tau$'\textsubscript{n} are distinct outcomes of the same action; and
	\item there exist two distinct goal trajectories $\tau$ and $\tau$' and outcome o $\in$ $\tau$ and o' $\in$ $\tau$' of two distinct actions a = act(o) and a'= act(o') such that executing a strictly decreases the maximum probability of reaching a state where a' can be executed.
\end{itemize}

Problems that fail to be \emph{probabilistically interesting} are actually classical planning problems in disguise and for those replanners are more appropriate. A replanner can also perform optimally for probabilistically interesting planning problems whenever the `most promising' goal trajectory is the correct thing to do. They create several domains to explore the issue of replanning vs\@. probabilistic planning.

Following existing domains were said to be probabilistically interesting:
\begin{itemize}
	\item Drive
	\item Exploding blocksworld
	\item Pitchcatch
	\item Schedule
	\item Tireworld
\end{itemize}

They conclude by stating that the negative perception of probabilistic planning by the ICAPS community was inaccurate. They state that a synthesis of planning and replanning techniques could make a larger number of probabilistic planning problems practicably solvable than is currently possible.

%---------------------------------------------------------------------

\section{Conclusion}

\begin{itemize}
	\item Conclude on the success of the 2 approaches
	\item Conclude the work done in the paper
	\item Hint to some future work developments
\end{itemize}

%\section{Some papers}
%
%\begin{itemize}
%	\item \cite{Hoffmann01theff}
%	\item \cite{FFReplan}
%	\item \cite{teichteil2010incremental}
%	\item \cite{yoon2008probabilistic}
%	\item \cite{teichteil2012fast}
%	\item \cite{bonet2011planning}
%	\item \cite{taig2013translation}
%	\item \cite{martin2013progressive}
%	\item \cite{taig2013conformant}
%	\item \cite{2013workshop}
%	\item \cite{nebel2013much}
%	\item \cite{little2007probvsreplan}
%	\item \cite{keller2012prost}
%	\item \cite{kolobov2012glutton}
%	\item \cite{sanner2008rddl}
%	\item \cite{kolobov2012gourmand}
%\end{itemize}


\bibliographystyle{plain}
\bibliography{paper}

\end{document}
