\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{fixltx2e}

\usepackage{url}
\urldef{\mailsa}\path|{i.c.t.m.speek, a.c.stolwijk}@student.tudelft.nl|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip%
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter% start of an individual contribution

% first the title is needed
\title{Presentation Plan\\
FF-Replan: \& RFF\@: Exploiting classical AI planning for uncertain and probabilistic domains}

% a short form should be given in case it is too long for the running head
\titlerunning{Presentation Plan}

\author{I.C.T.M Speek, A.C. Stolwijk}

%
\authorrunning{Presentation Plan Seminar Algorithms FF-replan \& RFF}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Seminar Algorithms, Embedded Software,
Msc Embedded Systems,\\
Delft University of Technology\\
\mailsa\\
}

%\toctitle{Lecture Notes in Computer Science}
%\tocauthor{Authors' Instructions}

\maketitle

\section{Summary of given papers}

\subsection{FF-Replan: A Baseline for Probabilistic Planning}

At the first international probabilistic planning competition IPPC-04, most entries were Markov Decision Processes. The winner, FF-Replan, however was based on deterministic planning techniques.

FF-Replan is an action selection algorithm for online planning in probabilistic domains. Given a domain description, a goal and an initial state as a probabilistic planning problem, FF-Replan generates a deterministic planning problem. It determinizes the input domain at the start of the planning into a deterministic different domain. They consider two methods of determinization:
\begin{enumerate}
	\item \emph{Single-outcome determinization}: selects one outcome for each probabilistic construct using a variety of heuristics. The performance strongly depends on the methods for choosing the outcomes.
	\item \emph{All-outcomes determinization}: considers every probabilistic outcome as a distinct deterministic action. For each possible outcome  FF-Replan generates one seperate action for each effect. For nested probabilities, the all-outcomes procedure is applied recursively. This implies that FF-Replan will ceise selecting actions whenever it enters a dead state. However when given no information, all probabilistic effects will be considered as equal which is a potential weakness.
\end{enumerate}

After determinization FF-Replan then uses the deterministic FF planner to compute an ordered plan for the generated deterministic problem. This process is repeated when countering an unexpected state untill a goal state is reached. It maintains a partial state-action mapping using a hash-table which is initially empty. Whenever FF-Replan encounters a state which is not in the table, it determinizes the problem and synthesizes a plan using FF. It then simulates the plan according to the deterministic action definitions resulting in a state-action sequence whose pairs are put in the hash table. The first action of the plan is then executed in the environment, returning a new state. This partial policy is this produces in an online fashion. It would be valuable to look into incremental planning approaches that don't create a deterministic plan from scratch whenever an unexpected state is encountered or reusing previous planning effort.

FF-Replan has some obvious shortcomings as it first determinizes the input domain, removing all probabilistic information from the problem and then synthesizes a plan. If an unexpected state should occur whilst executing, the planner replans in the same determinization. It also does not consider multiple potential effects if an action.

By relying on search, FF-Replan could function with enormous speed and efficiency. However when dealing with difficult probabilistic problems attempting to span the spectrum of complexity ranging from deterministic problems to highly stochastic problems, FF-Replan is expected to come short. Because FF-Replan is developed hoping to inspire extensions and insight into the approach and planning domains, the authors are hoping to achieve this.

\subsection{Incremental Plan Aggregation for Generating Policies in MDPs}

Markov Decision Processes (MDPs) is the best known formalism for planning under
uncertainty. It can however be computationally intensive if large parts of the
policies are tried to optimize at once. Most recent MDP planning techniques are
using the following steps as a basis approach:

\begin{enumerate}
	\item Take MDP problem
	\item ``Determinize''  it into a classical planning problem
	\item Generate sequence of actions that represent a possible execution
		path
	\item MDP planner executes plan:
		\begin{itemize}
			\item If goal is reached, reports success
			\item Otherwise algorithm generates a new plan from failed state of
				the world
		\end{itemize}
\end{enumerate}

There are some issues with this approach:

\begin{itemize}
	\item It stops after the first computed path to a goal which is likely to fail
	\item May result in generating the same path over and over again
	\item Only generates an execution path, not a policy
\end{itemize}

The authors identified four aspects of policy generation for MDP planning
problems using a classical planning algorithm:

\begin{enumerate}
	\item How to determinize the MDP planning problem
	\item which plan (initial state, goal) to ask for
	\item How to incrementally aggregate the plans by the classical planner
		into a solution policy
	\item How to combine this coherently
\end{enumerate}

The paper presents the Robust FF (\texttt{RFF}) planner, which should solve the
issues. \texttt{RFF} first generates an initial plan from the initial state to
some goal using \texttt{FF} on a determination of the MDP\@. There are several
strategies to do this. \textsc{MostProbableOutcome} (MPO) only uses the action
from a state with the highest probability. \textsc{AllOutcomes} (AO) uses all
actions from a state. Then for each state in the plan, the successors of that
state are added too. This is expanding the execution structure. For the new
state the probability to reach this state should be higher than a certain
threshold and if \texttt{FF} does not return a failure for this state and a
certain goal. The goals can be chosen in three different ways, using
\textsc{ProblemGoal}, which uses the goals of the MDP, \textsc{RandomGoal},
which randomly picks some states from the policy graph and \textsc{BestGoal}
which picks the best states from the policy graph. The calculation of the
probability to a state from the initial state can be costly. To optimize this
in \texttt{RFF} a Monte-Carlo (MC) simulation is used. \texttt{RFF} terminates
if either the global probability is lower than the threshold, or there are no
new terminal states to be added.

Three theorems, one lemma and one corollary about MDP problems using
\texttt{RFF} are given:

\begin{description}
	\item[Lemma 1] For every MDP planning problem RFF terminates in
		finite time
	\item[Theorem 1] (Soundness of \texttt{RFF\textsubscript{MPO}}). For every
		MDP planning problem, every solution that
		\texttt{RFF\textsubscript{MPO}} finds is correct
	\item[Theorem 2] (Completeness of \texttt{RFF\textsubscript{AO}}). For
		every MDP planning problem, if a solution exists, it is found by
		\texttt{RFF\textsubscript{AO}} or returns a failure
	\item[Corollary 1] (Soundness of \texttt{RFF\textsubscript{AO}}). For every
		MDP planning problem, every solution that
		\texttt{RFF\textsubscript{AO}} finds is correct
	\item[Theorem 3] If there are no unsolvable states in the MDP, then the
		probability of success of any solution found by \texttt{RFF} is
		higher than $1 - \rho$ where $\rho$ is the threshold probability
\end{description}

For the IPC-08 the authors have implemented \texttt{RFF} in C++ to perform
experiments. It performed experiments on the ``fully-observable probabilistic
(FOP) planning track''. At IPC-08 it performed better than other competitors.
The experiments confirmed Theorem 3, a higher threshold mean that \texttt{RFF}
will generate policies that are more prone to replanning.

In the experiments different Goal-Selection strategies were tested.
\textsc{ProblemGoal} (PG) performed the worst. \textsc{RandomGoal} performed
better and \textsc{BestGoal} generally produced higher quality goals, but also
took more time.

The different ways of determinizing MDPs was also tested. It turned out that
\textsc{AllOutcomes} solve much larger problems. However
\textsc{MostProbableOutcome} generally solve more problems in less time.

\section{Questions that arose}

\begin{enumerate}
	\item Are there scenarios where the optional step for optimizing the
		shortest stochastic path is beneficial, so where the quality
		improves enough to compensate the extra computational cost?
	\item -
\end{enumerate}

\section{Criticism on the contribution of the paper}

\section{Relevant literature on the subject}

\begin{itemize}
	\item Teichteil-Koenigsbuch, Florent, Guillaume Infantes, and Ugur Kuter. ``RFF: A robust, FF-based MDP planning algorithm for generating policies with low probability of failure.'' \emph{Sixth International Planning Competition at ICAPS} 8 (2008).
	\item Weld, Andrey Kolobov Mausam Daniel S. ``Determinize, Solve, and Generalize: Classical Planning for MDP Heuristics.''
\end{itemize}


\section{Presentation setup}

\begin{itemize}
	\item Introduction
	\item Brief introduction to prior knowledge
	\item Summary of the papers
	\item Advantages and disadvantages
	\item Comparison with ideas of other papers
	\item Some initial ideas of improvements
\end{itemize}

\end{document}
