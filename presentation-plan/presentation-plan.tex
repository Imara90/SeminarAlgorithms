\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{fixltx2e}

\usepackage{url}
\urldef{\mailsa}\path|{i.c.t.m.speek, a.c.stolwijk}@student.tudelft.nl|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip%
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter% start of an individual contribution

% first the title is needed
\title{Presentation Plan\\
FF-Replan: \& RFF\@: Exploiting classical AI planning for uncertain and probabilistic domains}

% a short form should be given in case it is too long for the running head
\titlerunning{Lecture Notes in Computer Science: Authors' Instructions}

\author{I.C.T.M Speek, A.C. Stolwijk}

%
\authorrunning{Presentation Plan Seminar Algorithms FF-replan \& RFF}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{Seminar Algorithms, Embedded Software,
Msc Embedded Systems,\\
Delft University of Technology\\
\mailsa\\
}

\toctitle{Lecture Notes in Computer Science}
\tocauthor{Authors' Instructions}
\maketitle

\section{Summary of given papers}

\subsection{FF-Replan: A Baseline for Probabilistic Planning}

At the first international probabilistic planning competition IPPC-04, most
entries were Markov Decision Processes. The winner, FF-Replan, however was
based on deterministic planning techniques.

By relying on search, FF-Replan could function with enormous speed and efficiency.

Until the paper there have been two IPPC conferences.

\subsection{Incremental Plan Aggregation for Generating Policies in MDPs}

Markov Decision Processes (MDPs) is the best known formalism for planning under
uncertainty. It can however be computationally intensive if large parts of the
policies are tried to optimize at once. Most recent MDP planning techniques are
using the following steps as a basis approach:

\begin{enumerate}
	\item Take MDP problem
	\item ``Determinize''  it into a classical planning problem
	\item Generate sequence of actions that represent a possible execution
		path
	\item MDP planner executes plan:
		\begin{itemize}
			\item If goal is reached, reports success
			\item Otherwise algorithm generates a new plan from failed state of
				the world
		\end{itemize}
\end{enumerate}

There are some issues with this approach:

\begin{itemize}
	\item It stops after the first computed path to a goal which is likely to fail
	\item May result in generating the same path over and over again
	\item Only generates an execution path, not a policy
\end{itemize}

The authors identified four aspects of policy generation for MDP planning
problems using a classical planning algorithm:

\begin{enumerate}
	\item How to determinize the MDP planning problem
	\item which plan (initial state, goal) to ask for
	\item How to incrementally aggregate the plans by the classical planner
		into a solution policy
	\item How to combine this coherently
\end{enumerate}

The paper presents the Robust FF (\texttt{RFF}) planner, which should solve the
issues. \texttt{RFF} first generates an initial plan from the initial state to
some goal using \texttt{FF} on a determination of the MDP\@. Then for each
state in the plan, the successors of that state are added too. This is
expanding the execution structure. For the new state the probability to reach
this state should be higher than a certain threshold and if \texttt{FF} does
not return a failure for this state and a certain goal. The calculation of the
probability to a state from the initial state can be costly. To optimize this
in \texttt{RFF} a Monte-Carlo (MC) simulation is used. \texttt{RFF} terminates
if either the global probability is lower than the threshold, or there are no
new terminal states to be added.

Three theorems, one lemma and one corollary about MDP problems using
\texttt{RFF} are given:

\begin{description}
	\item[Lemma 1] For every MDP planning problem RFF terminates in
		finite time
	\item[Theorem 1] (Soundness of \texttt{RFF\textsubscript{MPO}}). For every
		MDP planning problem, every solution that
		\texttt{RFF\textsubscript{MPO}} finds is correct
	\item[Theorem 2] (Completeness of \texttt{RFF\textsubscript{AO}}). For
		every MDP planning problem, if a solution exists, it is found by
		\texttt{RFF\textsubscript{AO}} or returns a failure
	\item[Corollary 1] (Soundness of \texttt{RFF\textsubscript{AO}}). For every
		MDP planning problem, every solution that
		\texttt{RFF\textsubscript{AO}} finds is correct
	\item[Theorem 3] If there are no unsolvable states in the MDP, then the
		probability of success of any solution found by \texttt{RFF} is
		higher than $1 - \rho$ where $\rho$ is the threshold probability
\end{description}


\section{Questions that arose}

\section{Criticism on the contribution of the paper}

\section{Relevant literature on the subject}

\section{Presentation setup}

\end{document}
